{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Preliminaries"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T11:10:09.544064Z","iopub.status.busy":"2022-05-10T11:10:09.543377Z","iopub.status.idle":"2022-05-10T11:13:42.403673Z","shell.execute_reply":"2022-05-10T11:13:42.402650Z","shell.execute_reply.started":"2022-05-10T11:10:09.544015Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils import data\n","import wandb\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","from sklearn.model_selection import train_test_split\n","from keras.layers import Embedding, Dense, LSTM, Dropout\n","from keras.losses import BinaryCrossentropy\n","from keras.models import Sequential\n","from keras.optimizers import adam_v2\n","from keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["config = dict()\n","\n","config['path_workspace'] = 'C:\\\\Users\\\\SafontAndreu\\\\Workspace\\\\Visual Studio Code\\\\ps_hiv\\\\'\n","config['path_inputs'] = 'C:\\\\Users\\\\SafontAndreu\\\\'\n","\n","config['path_database'] = config.get('path_workspace') + 'data\\\\'\n","config['path_models'] = config.get('path_inputs') + 'Models\\\\'\n","config['path_results'] = config.get('path_inputs') + 'Results\\\\'\n","config['path_ontology'] = config.get('path_inputs') + 'Ontology\\\\'"]},{"cell_type":"markdown","metadata":{},"source":["# Data"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["#X_name = 'PR Seq'\n","#X_name = 'RT Seq'\n","#X_name = 'Seq' # concatenate sequences\n","#X_name = 'Count' # concatenate counts\n","X_name = 'All' # concatenate viral/ct load with sequences\n","\n","y_name = 'Resp' # binary prognosis"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T11:13:42.406280Z","iopub.status.busy":"2022-05-10T11:13:42.405902Z","iopub.status.idle":"2022-05-10T11:13:43.368004Z","shell.execute_reply":"2022-05-10T11:13:43.367271Z","shell.execute_reply.started":"2022-05-10T11:13:42.406248Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PatientID</th>\n","      <th>Resp</th>\n","      <th>PR Seq</th>\n","      <th>RT Seq</th>\n","      <th>VL-t0</th>\n","      <th>CD4-t0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>CCTCAAATCACTCTTTGGCAACGACCCCTCGTCCCAATAAGGATAG...</td>\n","      <td>CCCATTAGTCCTATTGAAACTGTACCAGTAAAGCTAAAGCCAGGAA...</td>\n","      <td>4.3</td>\n","      <td>145</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGATAG...</td>\n","      <td>CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...</td>\n","      <td>3.6</td>\n","      <td>224</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGGTAG...</td>\n","      <td>CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...</td>\n","      <td>3.2</td>\n","      <td>1017</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAGGATAG...</td>\n","      <td>CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...</td>\n","      <td>5.7</td>\n","      <td>206</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAGTAAAGATAG...</td>\n","      <td>CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...</td>\n","      <td>3.5</td>\n","      <td>572</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PatientID  Resp                                             PR Seq  \\\n","0          1     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCCCAATAAGGATAG...   \n","1          2     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGATAG...   \n","2          3     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGGTAG...   \n","3          4     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAGGATAG...   \n","4          5     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAGTAAAGATAG...   \n","\n","                                              RT Seq  VL-t0  CD4-t0  \n","0  CCCATTAGTCCTATTGAAACTGTACCAGTAAAGCTAAAGCCAGGAA...    4.3     145  \n","1  CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    3.6     224  \n","2  CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    3.2    1017  \n","3  CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    5.7     206  \n","4  CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    3.5     572  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train_data = df = pd.read_csv(config.get('path_database')+'training_data.csv', header=0, index_col=False, encoding='utf-8', low_memory=False)\n","test_data = df = pd.read_csv(config.get('path_database')+'test_data_mod.csv', header=0, index_col=False, encoding='utf-8', low_memory=False)\n","sol_data = df = pd.read_csv(config.get('path_database')+'hivprogression_solution.csv', header=0, index_col=False, encoding='utf-8', low_memory=False)\n","train_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenize and vocab"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import collections\n","\n","def tokenize(seqs):\n","    return [tokenize_line(seq) for seq in seqs]\n","\n","def tokenize_line(seq):\n","    if not pd.isna(seq):\n","        return list(seq)\n","    return []\n","\n","class Vocab:\n","    def __init__(self, tokens):\n","        counter = count_corpus(tokens)\n","        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n","                                   reverse=True)\n","        self.idx_to_token = ['<unk>']\n","        self.token_to_idx = {token: idx\n","                             for idx, token in enumerate(self.idx_to_token)}\n","        for token, freq in self._token_freqs:\n","            if token not in self.token_to_idx:\n","                self.idx_to_token.append(token)\n","                self.token_to_idx[token] = len(self.idx_to_token) - 1\n","\n","    def __len__(self):\n","        return len(self.idx_to_token)\n","\n","    def __getitem__(self, tokens):\n","        if not isinstance(tokens, (list, tuple)):\n","            return self.token_to_idx.get(tokens, self.unk)\n","        return [self.__getitem__(token) for token in tokens]\n","\n","    def to_tokens(self, indices):\n","        if not isinstance(indices, (list, tuple)):\n","            return self.idx_to_token[indices]\n","        return [self.idx_to_token[index] for index in indices]\n","\n","    @property\n","    def unk(self): \n","        return 0\n","\n","    @property\n","    def token_freqs(self):\n","        return self._token_freqs\n","\n","def count_corpus(tokens):\n","    if len(tokens) == 0 or isinstance(tokens[0], list):\n","        tokens = [token for line in tokens for token in line]\n","    return collections.Counter(tokens)"]},{"cell_type":"markdown","metadata":{},"source":["## Data preprocessing (training)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique nucleotides in PR Sequence =  ABCDGHKMNRSTVWY\n","Min. =  216\n","Max. =  297\n","Unique nucleotides in RT Sequence =  ABCDGHKMNRSTVWY\n","Min. =  579\n","Max. =  1482\n"]}],"source":["seq_pr = train_data['PR Seq'].values.tolist()\n","seq_pr = [b for b in seq_pr if isinstance(b, str)]\n","\n","seq_pr_unique = ''\n","for ele in seq_pr:\n","    if isinstance(ele, str):\n","        seq_pr_unique += ''.join(set(ele))\n","seq_pr_unique = ''.join(set(seq_pr_unique))\n","seq_pr_unique = ''.join(sorted(seq_pr_unique))\n","print('Unique nucleotides in PR Sequence = ', seq_pr_unique)\n","print('Min. = ', len(min(seq_pr, key=len)))\n","print('Max. = ', len(max(seq_pr, key=len)))\n","\n","\n","\n","seq_rt = train_data['RT Seq'].values.tolist()\n","seq_rt = [b for b in seq_rt if isinstance(b, str)]\n","\n","seq_rt_unique = ''\n","for ele in seq_rt:\n","    if isinstance(ele, str):\n","        seq_rt_unique += ''.join(set(ele))\n","seq_rt_unique = ''.join(set(seq_rt_unique))\n","seq_rt_unique = ''.join(sorted(seq_rt_unique))\n","print('Unique nucleotides in RT Sequence = ', seq_rt_unique)\n","print('Min. = ', len(min(seq_rt, key=len)))\n","print('Max. = ', len(max(seq_rt, key=len)))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T11:13:44.274531Z","iopub.status.busy":"2022-05-10T11:13:44.274285Z","iopub.status.idle":"2022-05-10T11:13:45.104694Z","shell.execute_reply":"2022-05-10T11:13:45.103813Z","shell.execute_reply.started":"2022-05-10T11:13:44.274504Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(1000, 4)\n","(1000, 7)\n"]}],"source":["all_features = train_data.iloc[:, 2:]\n","print(all_features.shape)\n","# one can assume if Seqs are not present it is a bad sign for survival\n","all_features[\"PR SeqNan\"] = all_features[\"PR Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\n","all_features[\"RT SeqNan\"] = all_features[\"RT Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\n","numeric_features = all_features.dtypes[(all_features.dtypes != 'object') & (all_features.dtypes != 'bool')].index\n","mean_numerical_features = all_features[numeric_features].mean()\n","std_numerical_features = all_features[numeric_features].std()\n","all_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / x.std() + 1e-4)\n","vt_mean = all_features[\"VL-t0\"].mean()\n","cd4_mean = all_features[\"CD4-t0\"].mean()\n","all_features[\"VL-t0\"] = all_features[\"VL-t0\"].fillna(vt_mean)\n","all_features[\"CD4-t0\"] = all_features[\"CD4-t0\"].fillna(cd4_mean)\n","all_features.head()\n","\n","# Add results\n","all_features[y_name] = train_data[y_name]\n","print(all_features.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Subsets (training)"]},{"cell_type":"markdown","metadata":{},"source":["#### Select input"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# PR Seq\n","tokens_pr = tokenize(all_features[\"PR Seq\"].values)\n","vocab_pr = Vocab(tokens_pr)\n","list(vocab_pr.token_to_idx.items())\n","all_features[\"PR Seq\"] = all_features[\"PR Seq\"].apply(lambda x: vocab_pr[tokenize_line(x)])\n","#all_features[\"PR Seq\"]\n","\n","# RT Seq\n","tokens_rt = tokenize(all_features[\"RT Seq\"].values)\n","vocab_rt = Vocab(tokens_rt)\n","list(vocab_rt.token_to_idx.items())\n","all_features[\"RT Seq\"] = all_features[\"RT Seq\"].apply(lambda x: vocab_rt[tokenize_line(x)])\n","#all_features[\"RT Seq\"]\n","\n","\n","# RT Seq\n","all_features[\"Seq\"] = all_features[\"PR Seq\"] + all_features[\"RT Seq\"]\n","\n","\n","# All\n","a_ = all_features[\"VL-t0\"]\n","b_ = all_features[\"CD4-t0\"]\n","c_ = all_features[\"Seq\"]\n","matrix = []\n","for i in range(len(a_)):\n","    row = [a_[i]] + [b_[i]] + c_[i]\n","    matrix.append(row)\n","all_features['All'] = matrix"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(750,)\n","(750,)\n","(250,)\n","(250,)\n"]}],"source":["X = all_features[X_name].values\n","y = all_features[y_name].values\n","\n","x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","print(x_train.shape)\n","print(y_train.shape)\n","print(x_valid.shape)\n","print(y_valid.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Data preprocessing (test)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(692, 4)\n","(692, 7)\n"]}],"source":["all_features_test = test_data.iloc[:, 2:]\n","print(all_features_test.shape)\n","# one can assume if Seqs are not present it is a bad sign for survival\n","all_features_test[\"PR SeqNan\"] = all_features_test[\"PR Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\n","all_features_test[\"RT SeqNan\"] = all_features_test[\"RT Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\n","numeric_features_test = all_features_test.dtypes[(all_features_test.dtypes != 'object') & (all_features_test.dtypes != 'bool')].index\n","mean_numerical_features_test = all_features_test[numeric_features_test].mean()\n","std_numerical_features_test = all_features_test[numeric_features_test].std()\n","all_features_test[numeric_features_test] = all_features_test[numeric_features_test].apply(lambda x: (x - x.mean()) / x.std() + 1e-4)\n","vt_mean_test = all_features_test[\"VL-t0\"].mean()\n","cd4_mean_test = all_features_test[\"CD4-t0\"].mean()\n","all_features_test[\"VL-t0\"] = all_features_test[\"VL-t0\"].fillna(vt_mean_test)\n","all_features_test[\"CD4-t0\"] = all_features_test[\"CD4-t0\"].fillna(cd4_mean_test)\n","all_features_test.head()\n","\n","# Add results\n","all_features_test[y_name] = test_data[y_name]\n","print(all_features_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Subsets (test)"]},{"cell_type":"markdown","metadata":{},"source":["#### Select input"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# PR Seq\n","tokens_pr_test = tokenize(all_features_test[\"PR Seq\"].values)\n","vocab_pr_test = Vocab(tokens_pr_test)\n","list(vocab_pr_test.token_to_idx.items())\n","all_features_test[\"PR Seq\"] = all_features_test[\"PR Seq\"].apply(lambda x: vocab_pr_test[tokenize_line(x)])\n","#all_features[\"PR Seq\"]\n","\n","# RT Seq\n","tokens_rt_test = tokenize(all_features_test[\"RT Seq\"].values)\n","vocab_rt_test = Vocab(tokens_rt_test)\n","list(vocab_rt_test.token_to_idx.items())\n","all_features_test[\"RT Seq\"] = all_features_test[\"RT Seq\"].apply(lambda x: vocab_rt_test[tokenize_line(x)])\n","#all_features[\"RT Seq\"]\n","\n","\n","# RT Seq\n","all_features_test[\"Seq\"] = all_features_test[\"PR Seq\"] + all_features_test[\"RT Seq\"]\n","\n","\n","# All\n","a_ = all_features_test[\"VL-t0\"]\n","b_ = all_features_test[\"CD4-t0\"]\n","c_ = all_features_test[\"Seq\"]\n","matrix_test = []\n","for i in range(len(a_)):\n","    row = [a_[i]] + [b_[i]] + c_[i]\n","    matrix_test.append(row)\n","all_features_test['All'] = matrix_test"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["x_test = all_features_test[X_name]\n","y_test = all_features_test[y_name]"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"markdown","metadata":{},"source":["## Parameters"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Max. seq length =  1781\n","Size of vocabular =  17\n"]}],"source":["class_weight = {0: 0.2, # no improvement (80%)\n","                1: 0.8} # improvement (20%)\n","\n","additional_metrics = ['accuracy']\n","batch_size = 8\n","embedding_output_dims = 8\n","loss_function = BinaryCrossentropy()\n","max_sequence_length = max(all_features[X_name].apply(len))\n","print('Max. seq length = ', max_sequence_length)\n","num_distinct_words = len(vocab_pr)+1\n","print('Size of vocabular = ', num_distinct_words)\n","epochs = 50\n","lr = 2e-3\n","optimizer = adam_v2.Adam(learning_rate=lr, decay=lr/epochs)\n","verbosity_mode = 1"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# Disable eager execution\n","#tf.compat.v1.disable_eager_execution()\n","\n","# Pad all sequences\n","padded_x_train = pad_sequences(x_train, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n","padded_x_valid = pad_sequences(x_valid, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_21 (Dense)            (None, 128)               228096    \n","                                                                 \n"," dropout_15 (Dropout)        (None, 128)               0         \n","                                                                 \n"," dense_22 (Dense)            (None, 64)                8256      \n","                                                                 \n"," dropout_16 (Dropout)        (None, 64)                0         \n","                                                                 \n"," dense_23 (Dense)            (None, 16)                1040      \n","                                                                 \n"," dropout_17 (Dropout)        (None, 16)                0         \n","                                                                 \n"," dense_24 (Dense)            (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 237,409\n","Trainable params: 237,409\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model = Sequential()\n","\n","if X_name == 'PR Seq' or X_name == 'RT Seq': # LSTM\n","    model.add(Embedding(num_distinct_words, embedding_output_dims, input_length=max_sequence_length))\n","    model.add(LSTM(60))\n","    model.add(Dense(max_sequence_length/2, activation='relu'))\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(16, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","else: # NN\n","    model.add(Dense(128, input_dim=max_sequence_length, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(16, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","\n","\n","# Compile the model\n","model.compile(optimizer=optimizer, loss=loss_function, metrics=additional_metrics)\n","\n","# Give a summary\n","#model.build((len(X[0]), 1)) # `input_shape` is the shape of the input data\n","                         # e.g. input_shape = (None, 32, 32, 3)\n","model.summary()"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","94/94 [==============================] - 1s 6ms/step - loss: 1.1765 - accuracy: 0.5387 - val_loss: 0.7241 - val_accuracy: 0.2160\n","Epoch 2/50\n","94/94 [==============================] - 0s 4ms/step - loss: 0.2310 - accuracy: 0.2293 - val_loss: 0.7086 - val_accuracy: 0.2160\n","Epoch 3/50\n","94/94 [==============================] - 0s 4ms/step - loss: 0.2228 - accuracy: 0.2027 - val_loss: 0.7063 - val_accuracy: 0.2160\n","Epoch 4/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2234 - accuracy: 0.2027 - val_loss: 0.7045 - val_accuracy: 0.2160\n","Epoch 5/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2236 - accuracy: 0.2013 - val_loss: 0.7058 - val_accuracy: 0.2160\n","Epoch 6/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.7022 - val_accuracy: 0.2160\n","Epoch 7/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6990 - val_accuracy: 0.2280\n","Epoch 8/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2231 - accuracy: 0.2013 - val_loss: 0.7018 - val_accuracy: 0.2160\n","Epoch 9/50\n","94/94 [==============================] - 1s 6ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.7008 - val_accuracy: 0.2160\n","Epoch 10/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6990 - val_accuracy: 0.2160\n","Epoch 11/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6986 - val_accuracy: 0.2160\n","Epoch 12/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6990 - val_accuracy: 0.2160\n","Epoch 13/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6986 - val_accuracy: 0.2160\n","Epoch 14/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6994 - val_accuracy: 0.2160\n","Epoch 15/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6991 - val_accuracy: 0.2160\n","Epoch 16/50\n","94/94 [==============================] - 1s 6ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6984 - val_accuracy: 0.2160\n","Epoch 17/50\n","94/94 [==============================] - 1s 6ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6990 - val_accuracy: 0.2160\n","Epoch 18/50\n","94/94 [==============================] - 1s 6ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6996 - val_accuracy: 0.2160\n","Epoch 19/50\n","94/94 [==============================] - 1s 6ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6971 - val_accuracy: 0.2160\n","Epoch 20/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6985 - val_accuracy: 0.2160\n","Epoch 21/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6977 - val_accuracy: 0.2160\n","Epoch 22/50\n","94/94 [==============================] - 1s 6ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6984 - val_accuracy: 0.2160\n","Epoch 23/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6971 - val_accuracy: 0.2160\n","Epoch 24/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6974 - val_accuracy: 0.2160\n","Epoch 25/50\n","94/94 [==============================] - 1s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6954 - val_accuracy: 0.2160\n","Epoch 26/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6976 - val_accuracy: 0.2160\n","Epoch 27/50\n","94/94 [==============================] - 1s 8ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6963 - val_accuracy: 0.2160\n","Epoch 28/50\n","94/94 [==============================] - 1s 6ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6990 - val_accuracy: 0.2160\n","Epoch 29/50\n","94/94 [==============================] - 1s 5ms/step - loss: 0.2230 - accuracy: 0.2453 - val_loss: 0.6968 - val_accuracy: 0.2160\n","Epoch 30/50\n","94/94 [==============================] - 1s 6ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6987 - val_accuracy: 0.2160\n","Epoch 31/50\n","94/94 [==============================] - 1s 5ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6973 - val_accuracy: 0.2160\n","Epoch 32/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6984 - val_accuracy: 0.2160\n","Epoch 33/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6967 - val_accuracy: 0.2160\n","Epoch 34/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6977 - val_accuracy: 0.2160\n","Epoch 35/50\n","94/94 [==============================] - 1s 7ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6974 - val_accuracy: 0.2160\n","Epoch 36/50\n","94/94 [==============================] - 1s 6ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6975 - val_accuracy: 0.2160\n","Epoch 37/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6971 - val_accuracy: 0.2160\n","Epoch 38/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6987 - val_accuracy: 0.2160\n","Epoch 39/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6971 - val_accuracy: 0.2160\n","Epoch 40/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6976 - val_accuracy: 0.2160\n","Epoch 41/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6957 - val_accuracy: 0.2160\n","Epoch 42/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6976 - val_accuracy: 0.2160\n","Epoch 43/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6980 - val_accuracy: 0.2160\n","Epoch 44/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6981 - val_accuracy: 0.2160\n","Epoch 45/50\n","94/94 [==============================] - 1s 6ms/step - loss: 0.2229 - accuracy: 0.2027 - val_loss: 0.6973 - val_accuracy: 0.2160\n","Epoch 46/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6965 - val_accuracy: 0.2160\n","Epoch 47/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6985 - val_accuracy: 0.2160\n","Epoch 48/50\n","94/94 [==============================] - 1s 6ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6965 - val_accuracy: 0.2160\n","Epoch 49/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6988 - val_accuracy: 0.2160\n","Epoch 50/50\n","94/94 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.2027 - val_loss: 0.6977 - val_accuracy: 0.2160\n"]}],"source":["# Train the model\n","history = model.fit(padded_x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=verbosity_mode, validation_data=(padded_x_valid, y_valid), class_weight=class_weight)"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test results - Loss: 0.6931788325309753 - Accuracy: 50.0%\n"]}],"source":["padded_inputs_test = pad_sequences(x_test, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n","\n","test_results = model.evaluate(padded_inputs_test, y_test, verbose=False)\n","print(f'Test results - Loss: {test_results[0]} - Accuracy: {100*test_results[1]}%')"]}],"metadata":{"interpreter":{"hash":"42e89516d249f0b2da6278eedde058b7558b40d85564a4e7660d0a7c55b09129"},"kernelspec":{"display_name":"Python 3.8.10 ('.venv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
