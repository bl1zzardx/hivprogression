{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Preliminaries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T11:10:09.544064Z","iopub.status.busy":"2022-05-10T11:10:09.543377Z","iopub.status.idle":"2022-05-10T11:13:42.403673Z","shell.execute_reply":"2022-05-10T11:13:42.402650Z","shell.execute_reply.started":"2022-05-10T11:10:09.544015Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils import data\n","import wandb\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","from sklearn.model_selection import train_test_split\n","from keras.layers import Embedding, Dense, LSTM, Dropout\n","from keras.losses import BinaryCrossentropy\n","from keras.models import Sequential\n","from keras.optimizers import adam_v2\n","from keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["config = dict()\n","\n","config['path_workspace'] = 'C:\\\\Users\\\\SafontAndreu\\\\Workspace\\\\Visual Studio Code\\\\ps_hiv\\\\'\n","\n","config['path_database'] = config.get('path_workspace') + 'data\\\\'"]},{"cell_type":"markdown","metadata":{},"source":["# Data"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#X_name = 'PR Seq'\n","#X_name = 'RT Seq'\n","#X_name = 'Seq' # concatenate sequences\n","#X_name = 'Count' # concatenate counts\n","#X_name = 'All' # concatenate viral/ct load with sequences\n","\n","y_name = 'Resp' # binary prognosis"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T11:13:42.406280Z","iopub.status.busy":"2022-05-10T11:13:42.405902Z","iopub.status.idle":"2022-05-10T11:13:43.368004Z","shell.execute_reply":"2022-05-10T11:13:43.367271Z","shell.execute_reply.started":"2022-05-10T11:13:42.406248Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PatientID</th>\n","      <th>Resp</th>\n","      <th>PR Seq</th>\n","      <th>RT Seq</th>\n","      <th>VL-t0</th>\n","      <th>CD4-t0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>CCTCAAATCACTCTTTGGCAACGACCCCTCGTCCCAATAAGGATAG...</td>\n","      <td>CCCATTAGTCCTATTGAAACTGTACCAGTAAAGCTAAAGCCAGGAA...</td>\n","      <td>4.3</td>\n","      <td>145</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGATAG...</td>\n","      <td>CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...</td>\n","      <td>3.6</td>\n","      <td>224</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGGTAG...</td>\n","      <td>CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...</td>\n","      <td>3.2</td>\n","      <td>1017</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAGGATAG...</td>\n","      <td>CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...</td>\n","      <td>5.7</td>\n","      <td>206</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAGTAAAGATAG...</td>\n","      <td>CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...</td>\n","      <td>3.5</td>\n","      <td>572</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PatientID  Resp                                             PR Seq  \\\n","0          1     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCCCAATAAGGATAG...   \n","1          2     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGATAG...   \n","2          3     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGGTAG...   \n","3          4     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAGGATAG...   \n","4          5     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAGTAAAGATAG...   \n","\n","                                              RT Seq  VL-t0  CD4-t0  \n","0  CCCATTAGTCCTATTGAAACTGTACCAGTAAAGCTAAAGCCAGGAA...    4.3     145  \n","1  CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    3.6     224  \n","2  CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    3.2    1017  \n","3  CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    5.7     206  \n","4  CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    3.5     572  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_data = df = pd.read_csv(config.get('path_database')+'training_data.csv', header=0, index_col=False, encoding='utf-8', low_memory=False)\n","test_data = df = pd.read_csv(config.get('path_database')+'test_data_mod.csv', header=0, index_col=False, encoding='utf-8', low_memory=False)\n","sol_data = df = pd.read_csv(config.get('path_database')+'hivprogression_solution.csv', header=0, index_col=False, encoding='utf-8', low_memory=False)\n","train_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenize and vocab"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import collections\n","\n","def tokenize(seqs):\n","    return [tokenize_line(seq) for seq in seqs]\n","\n","def tokenize_line(seq):\n","    if not pd.isna(seq):\n","        return list(seq)\n","    return []\n","\n","class Vocab:\n","    def __init__(self, tokens):\n","        counter = count_corpus(tokens)\n","        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n","                                   reverse=True)\n","        self.idx_to_token = ['<unk>']\n","        self.token_to_idx = {token: idx\n","                             for idx, token in enumerate(self.idx_to_token)}\n","        for token, freq in self._token_freqs:\n","            if token not in self.token_to_idx:\n","                self.idx_to_token.append(token)\n","                self.token_to_idx[token] = len(self.idx_to_token) - 1\n","\n","    def __len__(self):\n","        return len(self.idx_to_token)\n","\n","    def __getitem__(self, tokens):\n","        if not isinstance(tokens, (list, tuple)):\n","            return self.token_to_idx.get(tokens, self.unk)\n","        return [self.__getitem__(token) for token in tokens]\n","\n","    def to_tokens(self, indices):\n","        if not isinstance(indices, (list, tuple)):\n","            return self.idx_to_token[indices]\n","        return [self.idx_to_token[index] for index in indices]\n","\n","    @property\n","    def unk(self): \n","        return 0\n","\n","    @property\n","    def token_freqs(self):\n","        return self._token_freqs\n","\n","def count_corpus(tokens):\n","    if len(tokens) == 0 or isinstance(tokens[0], list):\n","        tokens = [token for line in tokens for token in line]\n","    return collections.Counter(tokens)"]},{"cell_type":"markdown","metadata":{},"source":["## Data preprocessing (training)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique nucleotides in PR Sequence =  ABCDGHKMNRSTVWY\n","Unique nucleotides in RT Sequence =  ABCDGHKMNRSTVWY\n"]}],"source":["seq_pr = train_data['PR Seq']\n","\n","seq_pr_unique = ''\n","for ele in seq_pr:\n","    if isinstance(ele, str):\n","        seq_pr_unique += ''.join(set(ele))\n","seq_pr_unique = ''.join(set(seq_pr_unique))\n","seq_pr_unique = ''.join(sorted(seq_pr_unique))\n","print('Unique nucleotides in PR Sequence = ', seq_pr_unique)\n","\n","\n","\n","seq_rt = train_data['RT Seq']\n","\n","seq_rt_unique = ''\n","for ele in seq_rt:\n","    if isinstance(ele, str):\n","        seq_rt_unique += ''.join(set(ele))\n","seq_rt_unique = ''.join(set(seq_rt_unique))\n","seq_rt_unique = ''.join(sorted(seq_rt_unique))\n","print('Unique nucleotides in RT Sequence = ', seq_rt_unique)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T11:13:44.274531Z","iopub.status.busy":"2022-05-10T11:13:44.274285Z","iopub.status.idle":"2022-05-10T11:13:45.104694Z","shell.execute_reply":"2022-05-10T11:13:45.103813Z","shell.execute_reply.started":"2022-05-10T11:13:44.274504Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(1000, 4)\n","(1000, 7)\n"]}],"source":["all_features = train_data.iloc[:, 2:]\n","print(all_features.shape)\n","# one can assume if Seqs are not present it is a bad sign for survival\n","all_features[\"PR SeqNan\"] = all_features[\"PR Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\n","all_features[\"RT SeqNan\"] = all_features[\"RT Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\n","numeric_features = all_features.dtypes[(all_features.dtypes != 'object') & (all_features.dtypes != 'bool')].index\n","mean_numerical_features = all_features[numeric_features].mean()\n","std_numerical_features = all_features[numeric_features].std()\n","all_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / x.std() + 1e-4)\n","vt_mean = all_features[\"VL-t0\"].mean()\n","cd4_mean = all_features[\"CD4-t0\"].mean()\n","all_features[\"VL-t0\"] = all_features[\"VL-t0\"].fillna(vt_mean)\n","all_features[\"CD4-t0\"] = all_features[\"CD4-t0\"].fillna(cd4_mean)\n","all_features.head()\n","\n","# Add results\n","all_features[y_name] = train_data[y_name]\n","print(all_features.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Subsets (training)"]},{"cell_type":"markdown","metadata":{},"source":["#### Select input"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["0      [4, 4, 4, 1, 2, 2, 1, 3, 2, 4, 4, 2, 1, 2, 2, ...\n","1      [4, 4, 4, 1, 2, 2, 1, 3, 2, 4, 4, 2, 1, 2, 2, ...\n","2      [4, 4, 4, 1, 2, 2, 1, 3, 2, 4, 4, 2, 1, 2, 2, ...\n","3      [4, 4, 4, 1, 2, 2, 1, 3, 2, 4, 4, 2, 1, 2, 2, ...\n","4      [4, 4, 4, 1, 2, 2, 1, 3, 2, 4, 4, 2, 1, 2, 2, ...\n","                             ...                        \n","995    [4, 4, 4, 1, 2, 2, 1, 3, 2, 4, 4, 2, 1, 2, 2, ...\n","996    [4, 4, 4, 1, 2, 6, 1, 3, 2, 4, 4, 2, 1, 2, 2, ...\n","997    [4, 4, 4, 1, 2, 6, 1, 3, 2, 4, 4, 2, 1, 2, 2, ...\n","998    [4, 4, 2, 1, 2, 2, 1, 3, 2, 4, 4, 2, 1, 2, 2, ...\n","999    [4, 4, 2, 1, 2, 2, 1, 3, 2, 4, 4, 2, 1, 2, 2, ...\n","Name: RT Seq, Length: 1000, dtype: object"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["X_name = 'PR Seq'\n","tokens = tokenize(all_features[X_name].values)\n","vocab = Vocab(tokens)\n","list(vocab.token_to_idx.items())\n","all_features[X_name] = all_features[X_name].apply(lambda x: vocab[tokenize_line(x)])\n","all_features[X_name]\n","\n","X_name = 'RT Seq'\n","tokens = tokenize(all_features[X_name].values)\n","vocab = Vocab(tokens)\n","list(vocab.token_to_idx.items())\n","all_features[X_name] = all_features[X_name].apply(lambda x: vocab[tokenize_line(x)])\n","all_features[X_name]"]},{"cell_type":"markdown","metadata":{},"source":["## Data preprocessing (test)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(692, 4)\n","(692, 7)\n"]}],"source":["all_features_test = test_data.iloc[:, 2:]\n","print(all_features_test.shape)\n","# one can assume if Seqs are not present it is a bad sign for survival\n","all_features_test[\"PR SeqNan\"] = all_features_test[\"PR Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\n","all_features_test[\"RT SeqNan\"] = all_features_test[\"RT Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\n","numeric_features_test = all_features_test.dtypes[(all_features_test.dtypes != 'object') & (all_features_test.dtypes != 'bool')].index\n","mean_numerical_features_test = all_features_test[numeric_features_test].mean()\n","std_numerical_features_test = all_features_test[numeric_features_test].std()\n","all_features_test[numeric_features_test] = all_features_test[numeric_features_test].apply(lambda x: (x - x.mean()) / x.std() + 1e-4)\n","vt_mean_test = all_features_test[\"VL-t0\"].mean()\n","cd4_mean_test = all_features_test[\"CD4-t0\"].mean()\n","all_features_test[\"VL-t0\"] = all_features_test[\"VL-t0\"].fillna(vt_mean_test)\n","all_features_test[\"CD4-t0\"] = all_features_test[\"CD4-t0\"].fillna(cd4_mean_test)\n","all_features_test.head()\n","\n","# Add results\n","all_features_test[y_name] = test_data[y_name]\n","print(all_features_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Subsets (test)"]},{"cell_type":"markdown","metadata":{},"source":["#### Select input"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["X_name = 'PR Seq'\n","tokens = tokenize(all_features_test[X_name].values)\n","vocab = Vocab(tokens)\n","list(vocab.token_to_idx.items())\n","all_features_test[X_name] = all_features_test[X_name].apply(lambda x: vocab[tokenize_line(x)])\n","#all_features_test[X_name]\n","\n","X_name = 'RT Seq'\n","tokens = tokenize(all_features_test[X_name].values)\n","vocab = Vocab(tokens)\n","list(vocab.token_to_idx.items())\n","all_features_test[X_name] = all_features_test[X_name].apply(lambda x: vocab[tokenize_line(x)])\n","#all_features_test[X_name]"]},{"cell_type":"markdown","metadata":{},"source":["## Select input/output\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["max_sequence_length_pr = max(all_features['PR Seq'].apply(len))\n","max_sequence_length_rt = max(all_features['RT Seq'].apply(len))\n","\n","padded_x_pr = pad_sequences(all_features['PR Seq'], maxlen=max_sequence_length_pr, value = 0.0) # 0.0 because it corresponds with <PAD>\n","padded_x_rt = pad_sequences(all_features['RT Seq'], maxlen=max_sequence_length_rt, value = 0.0) # 0.0 because it corresponds with <PAD>\n","padded_x_pr_test = pad_sequences(all_features_test['PR Seq'], maxlen=max_sequence_length_pr, value = 0.0) # 0.0 because it corresponds with <PAD>\n","padded_x_rt_test = pad_sequences(all_features_test['RT Seq'], maxlen=max_sequence_length_rt, value = 0.0) # 0.0 because it corresponds with <PAD>"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'numpy.ndarray'>\n"]}],"source":["print(type(padded_x_pr))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1000, 1)\n","(1000, 1)\n","<class 'numpy.ndarray'>\n"]}],"source":["# from numpy to keras tensors\n","import numpy as np\n","import keras.backend as K\n","\n","#padded_x_pr = K.constant(padded_x_pr)\n","#padded_x_rt = K.constant(padded_x_rt)\n","#padded_x_pr_test = K.constant(padded_x_pr_test)\n","#padded_x_rt_test = K.constant(padded_x_rt_test)\n","\n","x_vl_ = all_features['VL-t0'].values\n","x_cd_ = all_features['CD4-t0'].values\n","y_ = all_features[y_name].values\n","\n","x_vl = np.array(x_vl_).reshape(x_vl_.shape + (1,))\n","x_cd = np.array(x_cd_).reshape(x_cd_.shape + (1,))\n","y = np.array(y_).reshape(y_.shape + (1,))\n","\n","\n","#x_vl = K.constant(x_vl)\n","#x_cd = K.constant(x_cd)\n","\n","print(x_vl.shape)\n","print(x_cd.shape)\n","print(type(x_vl))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["\"x_vl = all_features['VL-t0'].values\\narr = np.array(x_vl).reshape(x_vl.shape + (1,))\\nprint(arr.shape)\""]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["'''x_vl = all_features['VL-t0'].values\n","arr = np.array(x_vl).reshape(x_vl.shape + (1,))\n","print(arr.shape)'''"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"markdown","metadata":{},"source":["## Parameters"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Max. seq length for PR =  297\n","Max. seq length for RT =  1482\n","Size of vocabular =  16\n"]}],"source":["class_weight = {0: 0.2, # no improvement (80%)\n","                1: 0.8} # improvement (20%)\n","\n","additional_metrics = ['accuracy']\n","batch_size = 32\n","embedding_output_dims = 8\n","loss_function = BinaryCrossentropy()\n","print('Max. seq length for PR = ', max_sequence_length_pr)\n","print('Max. seq length for RT = ', max_sequence_length_rt)\n","num_distinct_words = len(vocab)\n","print('Size of vocabular = ', num_distinct_words)\n","epochs = 5\n","lr = 2e-3\n","optimizer = adam_v2.Adam(learning_rate=lr, decay=lr/epochs)\n","validation_split = 0.20\n","verbosity_mode = 1\n","\n","# Disable eager execution\n","tf.compat.v1.disable_eager_execution()"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_5 (InputLayer)           [(None, 297)]        0           []                               \n","                                                                                                  \n"," input_6 (InputLayer)           [(None, 1482)]       0           []                               \n","                                                                                                  \n"," input_7 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," input_8 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 1781)         0           ['input_5[0][0]',                \n","                                                                  'input_6[0][0]',                \n","                                                                  'input_7[0][0]',                \n","                                                                  'input_8[0][0]']                \n","                                                                                                  \n"," dense_5 (Dense)                (None, 128)          228096      ['concatenate_1[0][0]']          \n","                                                                                                  \n"," dense_6 (Dense)                (None, 64)           8256        ['dense_5[0][0]']                \n","                                                                                                  \n"," dense_7 (Dense)                (None, 32)           2080        ['dense_6[0][0]']                \n","                                                                                                  \n"," dense_8 (Dense)                (None, 16)           528         ['dense_7[0][0]']                \n","                                                                                                  \n"," dense_9 (Dense)                (None, 1)            17          ['dense_8[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 238,977\n","Trainable params: 238,977\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["import tensorflow as tf\n","from keras.layers import *\n","from keras.models import Sequential, Model\n","import numpy as np\n","\n","'''padded_x_pr_tf = tf.convert_to_tensor(value=padded_x_pr, dtype='int32')\n","padded_x_rt_tf = tf.convert_to_tensor(value=padded_x_rt, dtype='int32')\n","x_vl_tf = tf.convert_to_tensor(value=x_vl, dtype='int32')\n","x_cd_tf = tf.convert_to_tensor(value=x_cd, dtype='int32')\n","\n","input_pr = Input(tensor=padded_x_pr_tf)\n","input_rt = Input(tensor=padded_x_rt_tf)\n","input_vl = Input(tensor=x_vl_tf)\n","input_cd = Input(tensor=x_cd_tf)'''\n","\n","'''(1000, 297)\n","(1000, 1482)\n","(1000, 1)\n","(1000, 1)\n","(1000, 1)'''\n","input_pr = Input(shape=(297,))\n","input_rt = Input(shape=(1482,))\n","input_vl = Input(shape=(1,))\n","input_cd = Input(shape=(1,))\n","\n","#input = Concatenate(axis=1)([input_vl, input_cd])\n","input = Concatenate(axis=1)([input_pr, input_rt, input_vl, input_cd])\n","\n","x = Dense(128)(input)\n","x = Dense(64)(x)\n","x = Dense(32)(x)\n","x = Dense(16)(x)\n","x = Dense(1)(x)\n","\n","#model = Model(inputs=[input_pr, input_rt, input_vl, input_cd], outputs=x)\n","model = Model(inputs=[input_pr, input_rt, input_vl, input_cd], outputs=x)\n","\n","model.summary()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 800 samples, validate on 200 samples\n","Epoch 1/5\n","800/800 [==============================] - 0s 462us/sample - loss: 2.9714 - accuracy: 0.8050 - val_loss: 4.2419 - val_accuracy: 0.7250\n","Epoch 2/5\n","800/800 [==============================] - 0s 372us/sample - loss: 2.9115 - accuracy: 0.8112 - val_loss: 4.2419 - val_accuracy: 0.7250\n","Epoch 3/5\n","800/800 [==============================] - 0s 303us/sample - loss: 2.9115 - accuracy: 0.8112 - val_loss: 4.2419 - val_accuracy: 0.7250\n","Epoch 4/5\n","800/800 [==============================] - 0s 318us/sample - loss: 2.9115 - accuracy: 0.8112 - val_loss: 4.2419 - val_accuracy: 0.7250\n","Epoch 5/5\n","800/800 [==============================] - 0s 339us/sample - loss: 2.9115 - accuracy: 0.8112 - val_loss: 4.2419 - val_accuracy: 0.7250\n"]}],"source":["# Disable eager execution\n","#tf.compat.v1.disable_eager_execution()\n","\n","# Compile the model\n","model.compile(optimizer=optimizer, loss=loss_function, metrics=additional_metrics)\n","\n","# Train the model\n","history = model.fit([padded_x_pr, padded_x_rt, x_vl, x_cd], y, epochs=epochs, batch_size=10, verbose=verbosity_mode, validation_split=0.2)"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'x_test' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\SafontAndreu\\Workspace\\Visual Studio Code\\ps_hiv\\anna.ipynb Cell 32'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SafontAndreu/Workspace/Visual%20Studio%20Code/ps_hiv/anna.ipynb#ch0000029?line=0'>1</a>\u001b[0m padded_inputs_test \u001b[39m=\u001b[39m pad_sequences(x_test, maxlen\u001b[39m=\u001b[39mmax_sequence_length, value \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m) \u001b[39m# 0.0 because it corresponds with <PAD>\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SafontAndreu/Workspace/Visual%20Studio%20Code/ps_hiv/anna.ipynb#ch0000029?line=2'>3</a>\u001b[0m test_results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(padded_inputs_test, y_test, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SafontAndreu/Workspace/Visual%20Studio%20Code/ps_hiv/anna.ipynb#ch0000029?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTest results - Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_results[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m - Accuracy: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m100\u001b[39m\u001b[39m*\u001b[39mtest_results[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"]}],"source":["padded_inputs_test = pad_sequences(x_test, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\n","\n","test_results = model.evaluate(padded_inputs_test, y_test, verbose=False)\n","print(f'Test results - Loss: {test_results[0]} - Accuracy: {100*test_results[1]}%')"]},{"cell_type":"markdown","metadata":{},"source":["# Template"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","import numpy as np\n","\n","input1 = Input(shape=(336,))\n","input2 = Input(shape=(336,))\n","input = Concatenate()([input1, input2])\n","x = Dense(64, activation = 'relu')(input)\n","x = Dense(16, activation = 'relu')(x)\n","x = Dense(1, activation = 'sigmoid')(x)\n","model = Model(inputs=[input1, input2], outputs=x)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.compile(\n","    optimizer = RMSprop(lr=0.02,rho=0.9,epsilon=None,decay=0),\n","    loss = 'mean_squared_error'\n",")\n","\n","\n","x1, x2 = np.random.randn(1000, 336), np.random.randn(1000, 336,)\n","y = np.random.randn(1000, 1)\n","print(x1.shape)\n","print(x2.shape)\n","print(y.shape)\n","\n","model.fit([x1, x2], y, epochs = 10)"]}],"metadata":{"interpreter":{"hash":"42e89516d249f0b2da6278eedde058b7558b40d85564a4e7660d0a7c55b09129"},"kernelspec":{"display_name":"Python 3.8.10 ('.venv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
