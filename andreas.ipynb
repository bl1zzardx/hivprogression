{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIV Progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from helpers.needlemanwunsch import nw\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "# from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into memory\n",
    "train_data = pd.read_csv('./hivprogression/training_data.csv')\n",
    "test_data = pd.read_csv('./hivprogression/test_data_mod.csv')\n",
    "\n",
    "# drop 80 rows because of NaNs\n",
    "train_data.dropna(subset=['PR Seq'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(data, seq_unit, seq_cutoff):\n",
    "    pr_splits = pd.DataFrame([re.findall('.'*seq_unit, x) for x in data['PR Seq']])\n",
    "    pr_splits.columns = [f'pr_{x}' for x in range(0, pr_splits.shape[1])]\n",
    "    \n",
    "    rt_splits = pd.DataFrame([re.findall('.'*seq_unit, x) for x in data['RT Seq']])\n",
    "    rt_splits.columns = [f'rt_{x}' for x in range(0,rt_splits.shape[1])]\n",
    "    data = pd.concat([data, pr_splits, rt_splits.iloc[:, :seq_cutoff]], axis=1)\n",
    "    return data.dropna()\n",
    "\n",
    "# data with RT seq starts to be NaN index 300 onwards\n",
    "seq_unit = 3 # as of biological nature\n",
    "seq_cutoff = 60\n",
    "train_data = split_sequences(train_data, seq_unit=seq_unit, seq_cutoff=seq_cutoff)\n",
    "test_data = split_sequences(test_data, seq_unit=seq_unit, seq_cutoff=seq_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare X and y\n",
    "\n",
    "# needed when model can not handle categorical data\n",
    "if False:\n",
    "    train_data.drop(['PR Seq', 'RT Seq'], axis=1, inplace=True) \n",
    "    test_data.drop(['PR Seq', 'RT Seq'], axis=1, inplace=True)\n",
    "\n",
    "X = train_data.drop(['PatientID', 'Resp'], axis=1)\n",
    "y = train_data['Resp'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and test data\n",
    "# X = pd.concat([train_data.drop(['PatientID', 'Resp'], axis=1), test_data.drop(['PatientID', 'Resp'], axis=1)])\n",
    "# y = pd.concat([pd.DataFrame(train_data['Resp'].values), pd.DataFrame(test_data['Resp'].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAA-GATAGGGGGGCAACT-AAAGGAAGCTCTATTAGATACAGGAGCAGATGATACAGTATTAGAAGACATG-G-AATTGCCAGG-AAGATGGAAACCAAAAAT-AATAGGGGGAATT\n",
      "CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGG-TAGGGGGGCAACTAAAA-GAAGCTCTATTAGATACAGGAGCAGATGATACAGTATTAGAAGACATGAGT--TTGCCAGGAAA-ATGGAAACCAAAAATG-ATAGGGGGAATT\n"
     ]
    }
   ],
   "source": [
    "print(nw(X['PR Seq'][1][0:150], X['PR Seq'][2][0:150], gap=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = X.select_dtypes(exclude=[np.number]).keys()\n",
    "\n",
    "# smote for all pipelines\n",
    "cat_col_index = X.columns.isin(categories) \n",
    "smotenc = SMOTENC(categorical_features=cat_col_index, random_state=42)\n",
    "X, y = smotenc.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on test partition of train data\n",
    "\n",
    "# best found params with optuna\n",
    "param = {'loss_function': 'CrossEntropy', 'learning_rate': 0.5956840896672528, 'l2_leaf_reg': 0.24629732316062503, 'colsample_bylevel': 0.09221224327044178, 'depth': 6, 'boosting_type': 'Plain', 'min_data_in_leaf': 3, 'one_hot_max_size': 6}\n",
    "\n",
    "catboost = CatBoostClassifier(**param, verbose=False, cat_features=categories.values)\n",
    "catboost.fit(x_train, y_train)\n",
    "\n",
    "y_pred = catboost.predict(x_test)\n",
    "print(accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "#print(cross_val_score(catboost, X, y, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply it to the actual test set\n",
    "# catboost.fit(X, y, verbose=False)\n",
    "yt_true = test_data['Resp'].values\n",
    "Xt = test_data.drop(['PatientID', 'Resp'], axis=1)\n",
    "yt = catboost.predict(Xt)\n",
    "accuracy_score(y_true=yt_true, y_pred=yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top_features(clf, feature_list, n_features):\n",
    "    # Get numerical feature importances\n",
    "    importances = list(clf.feature_importances_)\n",
    "    # List of tuples with features and respective importance\n",
    "    clf_feature_importance = [\n",
    "        (feature, round(importance, 5))\n",
    "        for feature, importance in zip(feature_list, importances)\n",
    "    ]\n",
    "    # Sort the feature importances by most important first\n",
    "    clf_feature_importance = sorted(\n",
    "        clf_feature_importance, key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "    # Print out the feature and importances\n",
    "    [\n",
    "        print(\"Variable: {:20} Importance: {}\".format(*pair))\n",
    "        for pair in clf_feature_importance[:n_features]\n",
    "    ]\n",
    "    \n",
    "    return clf_feature_importance\n",
    "\n",
    "display_top_features(catboost, x_train.columns, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq cutoff at 60\n",
    "\n",
    "### seq_unit=1: (85% / 57%)\n",
    "Variable: VL-t0                Importance: 13.86521\n",
    "Variable: pr_5                 Importance: 10.39549\n",
    "Variable: pr_27                Importance: 5.6937\n",
    "Variable: pr_159               Importance: 3.41444\n",
    "Variable: pr_109               Importance: 3.07486\n",
    "Variable: CD4-t0               Importance: 2.74411\n",
    "\n",
    "### seq_unit=3: (85% / 55%)\n",
    "Variable: VL-t0                Importance: 9.28557\n",
    "Variable: pr_9                 Importance: 7.53769\n",
    "Variable: pr_1                 Importance: 7.27599\n",
    "Variable: CD4-t0               Importance: 4.46829\n",
    "Variable: pr_36                Importance: 3.19629\n",
    "Variable: rt_59                Importance: 2.69794\n",
    "\n",
    "### seq_unit=6: (83% / 55%)\n",
    "Variable: VL-t0                Importance: 7.3187\n",
    "Variable: pr_4                 Importance: 5.75921\n",
    "Variable: pr_0                 Importance: 5.32162\n",
    "Variable: pr_18                Importance: 3.63693\n",
    "Variable: pr_32                Importance: 2.94053\n",
    "\n",
    "### seq_unit=9 (87% / 53%)\n",
    "Variable: VL-t0                Importance: 6.30741\n",
    "Variable: pr_3                 Importance: 4.18024\n",
    "Variable: pr_0                 Importance: 3.93211\n",
    "Variable: rt_34                Importance: 3.06481\n",
    "Variable: pr_12                Importance: 2.78733\n",
    "Variable: pr_15                Importance: 2.54252\n",
    "\n",
    "### seq_unit=12 (86% / 53%)\n",
    "Variable: VL-t0                Importance: 7.67306\n",
    "Variable: pr_0                 Importance: 6.37918\n",
    "Variable: pr_23                Importance: 4.42358\n",
    "Variable: rt_7                 Importance: 3.65446\n",
    "Variable: rt_25                Importance: 3.19963\n",
    "Variable: pr_20                Importance: 3.12038\n",
    "\n",
    "### seq_unit=15 (87% / 52%)\n",
    "Variable: VL-t0                Importance: 8.17894\n",
    "Variable: pr_0                 Importance: 5.20007\n",
    "Variable: rt_8                 Importance: 4.47401\n",
    "Variable: pr_9                 Importance: 4.14473\n",
    "Variable: rt_25                Importance: 3.44481\n",
    "Variable: CD4-t0               Importance: 3.25052"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq cutoff at 300\n",
    "\n",
    "### seq_unit=15: (55% / 59%)\n",
    "Variable: rt_44                Importance: 18.45517\n",
    "Variable: rt_60                Importance: 10.64342\n",
    "Variable: pr_6                 Importance: 7.49245\n",
    "\n",
    "### seq_unit=12:\n",
    "Variable: rt_74                Importance: 23.79079\n",
    "Variable: VL-t0                Importance: 23.77396\n",
    "Variable: rt_36                Importance: 18.20179\n",
    "Variable: rt_77                Importance: 18.20108\n",
    "\n",
    "### seq_unit=9:\n",
    "Variable: rt_70                Importance: 34.26852\n",
    "Variable: rt_138               Importance: 33.70472\n",
    "Variable: rt_91                Importance: 11.2332\n",
    "\n",
    "### seq_unit=6:\n",
    "Variable: pr_1                 Importance: 9.74413\n",
    "Variable: VL-t0                Importance: 9.33733\n",
    "Variable: pr_89                Importance: 4.40009\n",
    "Variable: CD4-t0               Importance: 2.76098\n",
    "Variable: rt_248               Importance: 1.8484\n",
    "\n",
    "### seq_unit=3:\n",
    "Variable: pr_1                 Importance: 9.74413\n",
    "Variable: VL-t0                Importance: 9.33733\n",
    "Variable: pr_89                Importance: 4.40009\n",
    "Variable: CD4-t0               Importance: 2.76098\n",
    "Variable: rt_248               Importance: 1.8484\n",
    "\n",
    "### seq_unit=2:\n",
    "Variable: VL-t0                Importance: 9.67627\n",
    "Variable: pr_2                 Importance: 8.45288\n",
    "Variable: CD4-t0               Importance: 6.85079\n",
    "Variable: rt_274               Importance: 3.6759\n",
    "Variable: pr_13                Importance: 3.59485\n",
    "\n",
    "### seq_unit=1: (80% / 53%)\n",
    "Variable: pr_5                 Importance: 16.89232\n",
    "Variable: VL-t0                Importance: 12.00046\n",
    "Variable: pr_27                Importance: 3.24595\n",
    "Variable: rt_291               Importance: 2.66615\n",
    "Variable: pr_211               Importance: 1.7225 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissions = pd.DataFrame({\n",
    "#     'PatientID': np.arange(1,yt_pred.shape[0]+1),\n",
    "#     'ResponderStatus': yt_pred\n",
    "# })\n",
    "# submissions.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    \n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    param = {\n",
    "        'loss_function': trial.suggest_categorical('loss_function', ['Logloss', 'CrossEntropy']),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e0),\n",
    "        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-2, 1e0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "        'depth': trial.suggest_int('depth', 6, 10),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 2, 20),\n",
    "        \"one_hot_max_size\": trial.suggest_int(\"one_hot_max_size\", 2, 20),  \n",
    "    }\n",
    "    \n",
    "    catboost = CatBoostClassifier(**param) #, cat_features=['PR Seq', 'RT Seq'])\n",
    "    catboost.fit(x_train, y_train, eval_set=[(x_test, y_test)], verbose=False, early_stopping_rounds=100)\n",
    "    # on test partition of train data\n",
    "    y_pred = catboost.predict(x_test)\n",
    "    \n",
    "    return accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "optune = False\n",
    "if optune:\n",
    "    # 3. Create a study object and optimize the objective function.\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=10, timeout=60)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "        \n",
    "# optuna.visualization.plot_param_importances(study)\n",
    "# optuna.visualization.plot_optimization_history(study)\n",
    "# optuna.visualization.plot_slice(study, params=['depth', 'learning_rate'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3737b9d0d059178761ba28ab21c61029430c82223de8c0557d97311517f7b08"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('ipykernel_py3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
