{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-11T15:33:17.571174Z","iopub.execute_input":"2022-05-11T15:33:17.571720Z","iopub.status.idle":"2022-05-11T15:33:17.590890Z","shell.execute_reply.started":"2022-05-11T15:33:17.571682Z","shell.execute_reply":"2022-05-11T15:33:17.590146Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"# Preliminaries","metadata":{}},{"cell_type":"code","source":"np.random.seed(0)\nrandom.seed(0)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:17.691158Z","iopub.execute_input":"2022-05-11T15:33:17.691455Z","iopub.status.idle":"2022-05-11T15:33:17.695739Z","shell.execute_reply.started":"2022-05-11T15:33:17.691423Z","shell.execute_reply":"2022-05-11T15:33:17.694960Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, Tensor\nfrom torch import optim\nfrom torch.utils import data\nimport wandb\n#wandb.init(project=\"HIV_kaggle\")","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:17.811111Z","iopub.execute_input":"2022-05-11T15:33:17.811918Z","iopub.status.idle":"2022-05-11T15:33:17.816191Z","shell.execute_reply.started":"2022-05-11T15:33:17.811872Z","shell.execute_reply":"2022-05-11T15:33:17.815273Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"Parse and look at first 5 rows","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/hivprogression/training_data.csv')\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:18.150698Z","iopub.execute_input":"2022-05-11T15:33:18.151233Z","iopub.status.idle":"2022-05-11T15:33:18.181100Z","shell.execute_reply.started":"2022-05-11T15:33:18.151191Z","shell.execute_reply":"2022-05-11T15:33:18.180245Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"labels = torch.tensor(train_data[\"Resp\"].values, dtype=torch.float)\nn_labels = labels.shape[0]\nn_labels","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:18.395935Z","iopub.execute_input":"2022-05-11T15:33:18.396409Z","iopub.status.idle":"2022-05-11T15:33:18.403616Z","shell.execute_reply.started":"2022-05-11T15:33:18.396360Z","shell.execute_reply":"2022-05-11T15:33:18.402597Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"n_train = train_data.shape[0]\nn_train","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:18.650611Z","iopub.execute_input":"2022-05-11T15:33:18.651232Z","iopub.status.idle":"2022-05-11T15:33:18.656086Z","shell.execute_reply.started":"2022-05-11T15:33:18.651183Z","shell.execute_reply":"2022-05-11T15:33:18.655557Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"We have to remove the first two columns","metadata":{}},{"cell_type":"code","source":"\nall_features = train_data.iloc[:, 2:]\n# one can assume if Seqs are not present it is a bad sign for survival\nall_features[\"PR SeqNan\"] = all_features[\"PR Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\nall_features[\"RT SeqNan\"] = all_features[\"RT Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\nnumeric_features = all_features.dtypes[(all_features.dtypes != 'object') & (all_features.dtypes != 'bool')].index\nmean_numerical_features = all_features[numeric_features].mean()\nstd_numerical_features = all_features[numeric_features].std()\nall_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / x.std() + 1e-4)\nvt_mean = all_features[\"VL-t0\"].mean()\ncd4_mean = all_features[\"CD4-t0\"].mean()\nall_features[\"VL-t0\"] = all_features[\"VL-t0\"].fillna(vt_mean)\nall_features[\"CD4-t0\"] = all_features[\"CD4-t0\"].fillna(cd4_mean)\nall_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:18.966242Z","iopub.execute_input":"2022-05-11T15:33:18.966882Z","iopub.status.idle":"2022-05-11T15:33:18.996545Z","shell.execute_reply.started":"2022-05-11T15:33:18.966847Z","shell.execute_reply":"2022-05-11T15:33:18.995341Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"def f_comma(my_str, group=3, char=','):\n    if not pd.isna(my_str):\n        my_str = str(my_str)\n        return char.join(my_str[i:i+group] for i in range(0, len(my_str), group))\n    return ''\n\nfor index, row in all_features.iterrows():\n    all_features['PR Seq'] = all_features['PR Seq'].replace([row['PR Seq']], f_comma(row['PR Seq']))\n    all_features['RT Seq'] = all_features['RT Seq'].replace([row['RT Seq']], f_comma(row['RT Seq']))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:19.071212Z","iopub.execute_input":"2022-05-11T15:33:19.071566Z","iopub.status.idle":"2022-05-11T15:33:20.257329Z","shell.execute_reply.started":"2022-05-11T15:33:19.071532Z","shell.execute_reply":"2022-05-11T15:33:20.256564Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize and Vocab","metadata":{}},{"cell_type":"code","source":"import collections\n\ndef tokenize(seqs):\n    return [tokenize_line(seq) for seq in seqs]\n\ndef tokenize_line(seq):\n    if not pd.isna(seq) and len(seq) > 0 and not pd.isna(seq[0]):\n        return list(seq.split(','))\n    return []\n\nclass Vocab:\n    def __init__(self, tokens):\n        counter = count_corpus(tokens)\n        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n                                   reverse=True)\n        self.idx_to_token = ['<unk>', '<mask>']\n        self.token_to_idx = {token: idx\n                             for idx, token in enumerate(self.idx_to_token)}\n        for token, freq in self._token_freqs:\n            if token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) - 1\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\n    @property\n    def unk(self): \n        return 0\n\n    @property\n    def token_freqs(self):\n        return self._token_freqs\n\ndef count_corpus(tokens):\n    if len(tokens) == 0 or isinstance(tokens[0], list):\n        tokens = [token for line in tokens for token in line]\n    return collections.Counter(tokens)\n\ntokens_pr = tokenize(all_features[\"PR Seq\"].values)\nvocab_pr = Vocab(tokens_pr)\nlist(vocab_pr.token_to_idx.items())","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:20.258847Z","iopub.execute_input":"2022-05-11T15:33:20.259055Z","iopub.status.idle":"2022-05-11T15:33:20.311224Z","shell.execute_reply.started":"2022-05-11T15:33:20.259030Z","shell.execute_reply":"2022-05-11T15:33:20.310532Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"all_features[\"PR Seq\"] = all_features[\"PR Seq\"].apply(lambda x: vocab_pr[tokenize_line(x)])\nall_features[\"PR Seq\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:20.312437Z","iopub.execute_input":"2022-05-11T15:33:20.312668Z","iopub.status.idle":"2022-05-11T15:33:20.378573Z","shell.execute_reply.started":"2022-05-11T15:33:20.312642Z","shell.execute_reply":"2022-05-11T15:33:20.377824Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"tokens_rt = tokenize(all_features[\"RT Seq\"].values)\nvocab_rt = Vocab(tokens_rt)\n#list(vocab_rt.token_to_idx.items())","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:20.380144Z","iopub.execute_input":"2022-05-11T15:33:20.380382Z","iopub.status.idle":"2022-05-11T15:33:20.465006Z","shell.execute_reply.started":"2022-05-11T15:33:20.380353Z","shell.execute_reply":"2022-05-11T15:33:20.464209Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"all_features[\"RT Seq\"] = all_features[\"RT Seq\"].apply(lambda x: vocab_rt[tokenize_line(x)])\nall_features[\"RT Seq\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:20.466255Z","iopub.execute_input":"2022-05-11T15:33:20.466461Z","iopub.status.idle":"2022-05-11T15:33:20.662928Z","shell.execute_reply.started":"2022-05-11T15:33:20.466436Z","shell.execute_reply":"2022-05-11T15:33:20.662068Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"# Network","metadata":{}},{"cell_type":"code","source":"import math\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.0, max_len: int = 1000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n                 nlayers: int, dropout: float = 0.3):\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, d_model)\n        self.d_model = d_model\n        self.init_weights()\n\n    def init_weights(self) -> None:\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n        src = self.encoder(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_mask)\n        #output = self.decoder(output)\n        return output\n    \ndef generate_square_subsequent_mask(sz: int) -> Tensor:\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:20.663953Z","iopub.execute_input":"2022-05-11T15:33:20.664162Z","iopub.status.idle":"2022-05-11T15:33:20.677458Z","shell.execute_reply.started":"2022-05-11T15:33:20.664136Z","shell.execute_reply":"2022-05-11T15:33:20.676619Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"# Performance optimization => batching","metadata":{}},{"cell_type":"markdown","source":"Padding text inputs","metadata":{}},{"cell_type":"code","source":"def padding_input(seqs):\n    # determine max length\n    max_length = 0\n    for seq in seqs:\n        max_length = max(max_length, len(seq))\n    result = torch.zeros((seqs.shape[0], max_length), dtype=int)\n    for i in range(seqs.shape[0]):\n        for j in range(len(seqs[i])):\n            result[i][j] = seqs[i][j]\n    return result, max_length","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:20.678725Z","iopub.execute_input":"2022-05-11T15:33:20.679165Z","iopub.status.idle":"2022-05-11T15:33:20.688278Z","shell.execute_reply.started":"2022-05-11T15:33:20.679133Z","shell.execute_reply":"2022-05-11T15:33:20.687713Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"pr_data, pr_length = padding_input(all_features[\"PR Seq\"].values)\nrt_data, rt_length = padding_input(all_features[\"RT Seq\"].values)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:20.689376Z","iopub.execute_input":"2022-05-11T15:33:20.690245Z","iopub.status.idle":"2022-05-11T15:33:22.411002Z","shell.execute_reply.started":"2022-05-11T15:33:20.690210Z","shell.execute_reply":"2022-05-11T15:33:22.410323Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"markdown","source":"Dataset and DataLoaders","metadata":{}},{"cell_type":"code","source":"training_size = int(0.7 * n_train)\ntrain_indexes = np.random.choice(n_train, training_size)\nnumerical_features = torch.tensor(all_features.iloc[:, 2:].astype('float').values, dtype=torch.float32)\ndataset_features = torch.utils.data.TensorDataset(numerical_features, labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:22.444295Z","iopub.execute_input":"2022-05-11T15:33:22.444760Z","iopub.status.idle":"2022-05-11T15:33:22.452941Z","shell.execute_reply.started":"2022-05-11T15:33:22.444717Z","shell.execute_reply":"2022-05-11T15:33:22.452242Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"markdown","source":"# TO FIX : SPLITTING DATASET","metadata":{}},{"cell_type":"code","source":"selected_index = torch.zeros((n_train, ), dtype=bool)\nselected_index[train_indexes] = True\nloader_features_train = torch.utils.data.DataLoader(dataset_features, batch_size=32, shuffle=False)\nloader_pr_train = torch.utils.data.DataLoader(pr_data, batch_size=32, shuffle=False)\nloader_rt_train = torch.utils.data.DataLoader(rt_data, batch_size=32, shuffle=False)\nselected_index = torch.ones((n_train, ), dtype=bool)\nselected_index[train_indexes] = False","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:22.454720Z","iopub.execute_input":"2022-05-11T15:33:22.455382Z","iopub.status.idle":"2022-05-11T15:33:22.464393Z","shell.execute_reply.started":"2022-05-11T15:33:22.455346Z","shell.execute_reply":"2022-05-11T15:33:22.463726Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def init_layer(m):\n    torch.nn.init.xavier_normal_(m.weight, gain=torch.nn.init.calculate_gain('tanh'))\n    torch.nn.init.constant_(m.bias, 0)\n    return m\n\nclass Network(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.dropout = nn.Dropout(p=0.3)\n        self.tanh = nn.Tanh()\n        self.ln1 = init_layer(nn.Linear(6, 12))\n        self.ln2 = init_layer(nn.Linear(12, 12))\n        self.ln3 = init_layer(nn.Linear(12, 1))\n        self.sigmoid = nn.Sigmoid()\n        self.pr_model = TransformerModel(len(vocab_pr), 32, 4, 64, 4)\n        self.rt_model = TransformerModel(len(vocab_rt), 32, 4, 64, 4)\n        self.decoder_pr = init_layer(nn.Linear(32, 1))\n        self.decoder_rt = init_layer(nn.Linear(32, 1))\n        \n    def forward(self, x):\n        x = self.dropout(self.tanh(self.ln1(x)))\n        x = self.dropout(self.tanh(self.ln2(x)))\n        return self.sigmoid(self.ln3(x))\n    \n    def forward_rt(self, x, mask):\n        return self.decoder_rt(self.rt_model(x, mask).mean(dim=1))\n    \n    def forward_pr(self, x, mask):\n        return self.decoder_pr(self.pr_model(x, mask).mean(dim=1))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:22.465697Z","iopub.execute_input":"2022-05-11T15:33:22.465895Z","iopub.status.idle":"2022-05-11T15:33:22.477486Z","shell.execute_reply.started":"2022-05-11T15:33:22.465870Z","shell.execute_reply":"2022-05-11T15:33:22.476734Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"!pip install mlm-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:22.478404Z","iopub.execute_input":"2022-05-11T15:33:22.479198Z","iopub.status.idle":"2022-05-11T15:33:31.222952Z","shell.execute_reply.started":"2022-05-11T15:33:22.479163Z","shell.execute_reply":"2022-05-11T15:33:31.221881Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"from mlm_pytorch import MLM\n\ndef self_init_layer(m):\n    m.weight.data.uniform_(-0.1, 0.1)\n    torch.nn.init.constant_(m.bias, 0)\n    return m\n\nclass myTransformer(nn.Module):\n    \n    def __init__(self, vocab_len=len(vocab_pr)):\n        super().__init__()\n        self.model = TransformerModel(vocab_len, 32, 4, 64, 4)\n        self.decoder = self_init_layer(nn.Linear(32, vocab_len))\n        \n    def forward(self, x, mask=None):\n        src_mask = generate_square_subsequent_mask(x.size(0))\n        return self.decoder(self.model(x, src_mask))\n\n\ntransformer_pr = myTransformer(len(vocab_pr))\ntrainer = MLM(\n    transformer_pr,\n    mask_token_id = 1,          # the token id reserved for masking\n    pad_token_id = 0,           # the token id for padding\n    mask_prob = 0.2,           # masking probability for masked language modeling\n    replace_prob = 0.90,        \n    mask_ignore_token_ids = [0]  \n)\nopt = torch.optim.Adam(trainer.parameters(), lr=3e-4)\nnb_iterations = 100\nfor j in range(nb_iterations):\n    training_loss = 0\n    for batch_idx, data in enumerate(zip(loader_pr_train, loader_rt_train, loader_features_train)):\n        opt.zero_grad()\n        prs, rts, (num_features, label) = data\n        loss = trainer(prs)\n        loss.backward()\n        opt.step()\n        training_loss += loss.item()\n    print(f'iter {j} training loss {training_loss}')\n    \ntransformer_rt = myTransformer(len(vocab_rt))\ntrainer = MLM(\n    transformer_rt,\n    mask_token_id = 1,          # the token id reserved for masking\n    pad_token_id = 0,           # the token id for padding\n    mask_prob = 0.2,           # masking probability for masked language modeling\n    replace_prob = 0.90,        \n    mask_ignore_token_ids = [0]  \n)\nopt = torch.optim.Adam(trainer.parameters(), lr=3e-4)\nnb_iterations = 100\nfor j in range(nb_iterations):\n    training_loss = 0\n    for batch_idx, data in enumerate(zip(loader_pr_train, loader_rt_train, loader_features_train)):\n        opt.zero_grad()\n        prs, rts, (num_features, label) = data\n        loss = trainer(rts)\n        loss.backward()\n        opt.step()\n        training_loss += loss.item()\n    print(f'iter {j} training loss {training_loss}')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:33:31.225996Z","iopub.execute_input":"2022-05-11T15:33:31.226365Z","iopub.status.idle":"2022-05-11T15:49:43.060215Z","shell.execute_reply.started":"2022-05-11T15:33:31.226320Z","shell.execute_reply":"2022-05-11T15:49:43.058953Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"net = Network()\nnet.pr_model = transformer_pr.model\nnet.rt_model = transformer_rt.model\n\ncriterion=torch.nn.BCELoss(reduction='none')\n\noptimizer = torch.optim.Adam(net.parameters(), lr=3e-4)\n\nnb_iterations = 1000\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, nb_iterations)\n\nnet.train()\nfor j in range(nb_iterations):\n    correctly_predicted = 0\n    total_prediction = 0\n    training_loss = 0\n    src_mask = generate_square_subsequent_mask(32)\n    for batch_idx, data in enumerate(zip(loader_pr_train, loader_rt_train, loader_features_train)):\n        optimizer.zero_grad()\n        prs, rts, (num_features, label) = data\n        if rts.size(0) != 32:  # only on last batch\n            src_mask = src_mask[:rts.size(0), :rts.size(0)]\n        prs = net.forward_pr(prs, src_mask)\n        rts = net.forward_rt(rts, src_mask)\n        x = torch.hstack((num_features, prs, rts))\n        output = net(x)\n        smoothed_label = label * 0.8 + 0.1\n        loss = criterion(output.view(-1), smoothed_label)\n        coefficient = torch.ones_like(loss)\n        coefficient[label == True] += 3\n        loss = loss * coefficient\n        loss = loss.mean()\n        loss.backward()\n        optimizer.step()\n        training_loss += loss.item()\n        predicted = torch.ge(output, 0.5).view(-1)\n        correctly_predicted += torch.sum(label == predicted)\n        total_prediction += output.shape[0]\n    scheduler.step()\n    print(f'iter {j} training loss {training_loss} accuracy training {correctly_predicted / total_prediction}')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:49:46.610712Z","iopub.execute_input":"2022-05-11T15:49:46.611146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test data","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/modded-test/test_data_mod.csv')\ntrain_data.head()\nlabels = torch.tensor(test_data[\"Resp\"].values, dtype=torch.float)\nn_labels = labels.shape[0]\nn_train = train_data.shape[0]\nall_features = test_data.iloc[:, 2:]\n# one can assume if Seqs are not present it is a bad sign for survival\nall_features[\"PR SeqNan\"] = all_features[\"PR Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\nall_features[\"RT SeqNan\"] = all_features[\"RT Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\nnumeric_features = all_features.dtypes[(all_features.dtypes != 'object') & (all_features.dtypes != 'bool')].index\nmean_numerical_features = all_features[numeric_features].mean()\nstd_numerical_features = all_features[numeric_features].std()\nall_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / x.std() + 1e-4)\nvt_mean = all_features[\"VL-t0\"].mean()\ncd4_mean = all_features[\"CD4-t0\"].mean()\nall_features[\"VL-t0\"] = all_features[\"VL-t0\"].fillna(vt_mean)\nall_features[\"CD4-t0\"] = all_features[\"CD4-t0\"].fillna(cd4_mean)\nall_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:49:43.064066Z","iopub.status.idle":"2022-05-11T15:49:43.064791Z","shell.execute_reply.started":"2022-05-11T15:49:43.064532Z","shell.execute_reply":"2022-05-11T15:49:43.064561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f_comma(my_str, group=3, char=','):\n    if not pd.isna(my_str):\n        my_str = str(my_str)\n        return char.join(my_str[i:i+group] for i in range(0, len(my_str), group))\n    return ''\n\nfor index, row in all_features.iterrows():\n    all_features['PR Seq'] = all_features['PR Seq'].replace([row['PR Seq']], f_comma(row['PR Seq']))\n    all_features['RT Seq'] = all_features['RT Seq'].replace([row['RT Seq']], f_comma(row['RT Seq']))\n    \nall_features[\"PR Seq\"] = all_features[\"PR Seq\"].apply(lambda x: vocab_pr[tokenize_line(x)])\nall_features[\"RT Seq\"] = all_features[\"RT Seq\"].apply(lambda x: vocab_rt[tokenize_line(x)])\npr_data, pr_length = padding_input(all_features[\"PR Seq\"].values)\nrt_data, rt_length = padding_input(all_features[\"RT Seq\"].values)\n\nnumerical_features = torch.tensor(all_features.iloc[:, 2:].astype('float').values, dtype=torch.float32)\ndataset_features = torch.utils.data.TensorDataset(numerical_features, labels)\nloader_features_train = torch.utils.data.DataLoader(dataset_features, batch_size=32, shuffle=False)\nloader_pr_train = torch.utils.data.DataLoader(pr_data, batch_size=32, shuffle=False)\nloader_rt_train = torch.utils.data.DataLoader(rt_data, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:49:43.065938Z","iopub.status.idle":"2022-05-11T15:49:43.066796Z","shell.execute_reply.started":"2022-05-11T15:49:43.066493Z","shell.execute_reply":"2022-05-11T15:49:43.066539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(net.state_dict(), 'model.pt')\nnet.eval()\ncorrectly_predicted = 0\ntotal_prediction = 0\nsrc_mask = generate_square_subsequent_mask(32)\nwith torch.no_grad():\n    for batch_idx, data in enumerate(zip(loader_pr_train, loader_rt_train, loader_features_train)):\n        prs, rts, (num_features, label) = data\n        if rts.size(0) != 32:  # only on last batch\n            src_mask = src_mask[:rts.size(0), :rts.size(0)]\n        prs = net.forward_pr(prs, src_mask)\n        rts = net.forward_rt(rts, src_mask)\n        x = torch.hstack((num_features, prs, rts))\n        output = net(x)\n        predicted = torch.ge(output, 0.5).view(-1)\n        correctly_predicted += torch.sum(label == predicted)\n        total_prediction += output.shape[0]\n    print(f'Test accuracy {correctly_predicted / total_prediction}')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T15:49:43.068319Z","iopub.status.idle":"2022-05-11T15:49:43.069001Z","shell.execute_reply.started":"2022-05-11T15:49:43.068725Z","shell.execute_reply":"2022-05-11T15:49:43.068755Z"},"trusted":true},"execution_count":null,"outputs":[]}]}