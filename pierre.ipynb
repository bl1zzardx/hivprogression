{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-12T09:47:20.966596Z","iopub.execute_input":"2022-05-12T09:47:20.966879Z","iopub.status.idle":"2022-05-12T09:47:20.977029Z","shell.execute_reply.started":"2022-05-12T09:47:20.966848Z","shell.execute_reply":"2022-05-12T09:47:20.976021Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"# Preliminaries","metadata":{}},{"cell_type":"code","source":"np.random.seed(0)\nrandom.seed(0)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:20.978361Z","iopub.execute_input":"2022-05-12T09:47:20.979937Z","iopub.status.idle":"2022-05-12T09:47:20.984672Z","shell.execute_reply.started":"2022-05-12T09:47:20.979895Z","shell.execute_reply":"2022-05-12T09:47:20.983703Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, Tensor\nfrom torch import optim\nfrom torch.utils import data\nimport wandb\n#wandb.init(project=\"HIV_kaggle\")","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:21.005649Z","iopub.execute_input":"2022-05-12T09:47:21.006011Z","iopub.status.idle":"2022-05-12T09:47:21.010455Z","shell.execute_reply.started":"2022-05-12T09:47:21.005978Z","shell.execute_reply":"2022-05-12T09:47:21.009316Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"Parse and look at first 5 rows","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/hivprogression/training_data.csv')\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:21.107004Z","iopub.execute_input":"2022-05-12T09:47:21.107431Z","iopub.status.idle":"2022-05-12T09:47:21.135327Z","shell.execute_reply.started":"2022-05-12T09:47:21.107377Z","shell.execute_reply":"2022-05-12T09:47:21.134498Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"labels = torch.tensor(train_data[\"Resp\"].values, dtype=torch.float)\nn_labels = labels.shape[0]\nn_labels","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:21.136908Z","iopub.execute_input":"2022-05-12T09:47:21.137127Z","iopub.status.idle":"2022-05-12T09:47:21.145434Z","shell.execute_reply.started":"2022-05-12T09:47:21.137099Z","shell.execute_reply":"2022-05-12T09:47:21.144581Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"n_train = train_data.shape[0]\nn_train","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:21.147398Z","iopub.execute_input":"2022-05-12T09:47:21.148075Z","iopub.status.idle":"2022-05-12T09:47:21.157395Z","shell.execute_reply.started":"2022-05-12T09:47:21.148029Z","shell.execute_reply":"2022-05-12T09:47:21.156469Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"We have to remove the first two columns","metadata":{}},{"cell_type":"code","source":"\nall_features = train_data.iloc[:, 2:]\n# one can assume if Seqs are not present it is a bad sign for survival\n#all_features[\"PR SeqNan\"] = all_features[\"PR Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\n#all_features[\"RT SeqNan\"] = all_features[\"RT Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\nnumeric_features = all_features.dtypes[(all_features.dtypes != 'object') & (all_features.dtypes != 'bool')].index\nmean_numerical_features = all_features[numeric_features].mean()\nstd_numerical_features = all_features[numeric_features].std()\nall_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / x.std() + 1e-4)\nvt_mean = all_features[\"VL-t0\"].mean()\ncd4_mean = all_features[\"CD4-t0\"].mean()\nall_features[\"VL-t0\"] = all_features[\"VL-t0\"].fillna(vt_mean)\nall_features[\"CD4-t0\"] = all_features[\"CD4-t0\"].fillna(cd4_mean)\nall_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:21.289376Z","iopub.execute_input":"2022-05-12T09:47:21.289705Z","iopub.status.idle":"2022-05-12T09:47:21.316837Z","shell.execute_reply.started":"2022-05-12T09:47:21.289671Z","shell.execute_reply":"2022-05-12T09:47:21.316134Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"def f_comma(my_str, group=3, char=','):\n    if not pd.isna(my_str):\n        my_str = str(my_str)\n        return char.join(my_str[i:i+group] for i in range(0, len(my_str), group))\n    return ''\n\nfor index, row in all_features.iterrows():\n    all_features['PR Seq'] = all_features['PR Seq'].replace([row['PR Seq']], f_comma(row['PR Seq']))\n    all_features['RT Seq'] = all_features['RT Seq'].replace([row['RT Seq']], f_comma(row['RT Seq']))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:21.318405Z","iopub.execute_input":"2022-05-12T09:47:21.319022Z","iopub.status.idle":"2022-05-12T09:47:22.617046Z","shell.execute_reply.started":"2022-05-12T09:47:21.318982Z","shell.execute_reply":"2022-05-12T09:47:22.616265Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize and Vocab","metadata":{}},{"cell_type":"code","source":"import collections\n\ndef tokenize(seqs):\n    return [tokenize_line(seq) for seq in seqs]\n\ndef tokenize_line(seq):\n    if not pd.isna(seq) and len(seq) > 0 and not pd.isna(seq[0]):\n        return list(seq.split(','))\n    return []\n\nclass Vocab:\n    def __init__(self, tokens):\n        counter = count_corpus(tokens)\n        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n                                   reverse=True)\n        self.idx_to_token = ['<unk>', '<mask>']\n        self.token_to_idx = {token: idx\n                             for idx, token in enumerate(self.idx_to_token)}\n        for token, freq in self._token_freqs:\n            if token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) - 1\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\n    @property\n    def unk(self): \n        return 0\n\n    @property\n    def token_freqs(self):\n        return self._token_freqs\n\ndef count_corpus(tokens):\n    if len(tokens) == 0 or isinstance(tokens[0], list):\n        tokens = [token for line in tokens for token in line]\n    return collections.Counter(tokens)\n\ntokens_pr = tokenize(all_features[\"PR Seq\"].values)\nvocab_pr = Vocab(tokens_pr)\nlist(vocab_pr.token_to_idx.items())","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:22.618636Z","iopub.execute_input":"2022-05-12T09:47:22.618888Z","iopub.status.idle":"2022-05-12T09:47:22.677991Z","shell.execute_reply.started":"2022-05-12T09:47:22.618852Z","shell.execute_reply":"2022-05-12T09:47:22.677329Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"all_features[\"PR Seq\"] = all_features[\"PR Seq\"].apply(lambda x: vocab_pr[tokenize_line(x)])\nall_features[\"PR Seq\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:22.680675Z","iopub.execute_input":"2022-05-12T09:47:22.680921Z","iopub.status.idle":"2022-05-12T09:47:22.755266Z","shell.execute_reply.started":"2022-05-12T09:47:22.680894Z","shell.execute_reply":"2022-05-12T09:47:22.754407Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"tokens_rt = tokenize(all_features[\"RT Seq\"].values)\nvocab_rt = Vocab(tokens_rt)\n#list(vocab_rt.token_to_idx.items())","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:22.757371Z","iopub.execute_input":"2022-05-12T09:47:22.757635Z","iopub.status.idle":"2022-05-12T09:47:22.838457Z","shell.execute_reply.started":"2022-05-12T09:47:22.757601Z","shell.execute_reply":"2022-05-12T09:47:22.837791Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"all_features[\"RT Seq\"] = all_features[\"RT Seq\"].apply(lambda x: vocab_rt[tokenize_line(x)])\nall_features[\"RT Seq\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:22.840730Z","iopub.execute_input":"2022-05-12T09:47:22.841226Z","iopub.status.idle":"2022-05-12T09:47:23.066970Z","shell.execute_reply.started":"2022-05-12T09:47:22.841187Z","shell.execute_reply":"2022-05-12T09:47:23.066241Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"# Network","metadata":{}},{"cell_type":"code","source":"import math\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n                 nlayers: int, dropout: float = 0.3):\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, d_model)\n        self.d_model = d_model\n        self.init_weights()\n\n    def init_weights(self) -> None:\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n        src = self.encoder(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_mask)\n        #output = self.decoder(output)\n        return output\n    \ndef generate_square_subsequent_mask(sz: int) -> Tensor:\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:23.068219Z","iopub.execute_input":"2022-05-12T09:47:23.068481Z","iopub.status.idle":"2022-05-12T09:47:23.082830Z","shell.execute_reply.started":"2022-05-12T09:47:23.068446Z","shell.execute_reply":"2022-05-12T09:47:23.082180Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"# Performance optimization => batching","metadata":{}},{"cell_type":"markdown","source":"Padding text inputs","metadata":{}},{"cell_type":"code","source":"def padding_input(seqs):\n    # determine max length\n    max_length = 0\n    for seq in seqs:\n        max_length = max(max_length, len(seq))\n    result = torch.zeros((seqs.shape[0], max_length), dtype=int)\n    for i in range(seqs.shape[0]):\n        for j in range(len(seqs[i])):\n            result[i][j] = seqs[i][j]\n    return result, max_length","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:23.084060Z","iopub.execute_input":"2022-05-12T09:47:23.084782Z","iopub.status.idle":"2022-05-12T09:47:23.093008Z","shell.execute_reply.started":"2022-05-12T09:47:23.084742Z","shell.execute_reply":"2022-05-12T09:47:23.092297Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"pr_data, pr_length = padding_input(all_features[\"PR Seq\"].values)\nrt_data, rt_length = padding_input(all_features[\"RT Seq\"].values)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:23.096215Z","iopub.execute_input":"2022-05-12T09:47:23.096406Z","iopub.status.idle":"2022-05-12T09:47:24.854746Z","shell.execute_reply.started":"2022-05-12T09:47:23.096382Z","shell.execute_reply":"2022-05-12T09:47:24.853871Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"Dataset and DataLoaders","metadata":{}},{"cell_type":"code","source":"training_size = int(0.7 * n_train)\ntrain_indexes = np.random.choice(n_train, training_size)\nnumerical_features = torch.tensor(all_features.iloc[:, 2:].astype('float').values, dtype=torch.float32)\ndataset_features = torch.utils.data.TensorDataset(numerical_features, labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:24.860885Z","iopub.execute_input":"2022-05-12T09:47:24.861293Z","iopub.status.idle":"2022-05-12T09:47:24.868688Z","shell.execute_reply.started":"2022-05-12T09:47:24.861256Z","shell.execute_reply":"2022-05-12T09:47:24.867749Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":"# TO FIX : SPLITTING DATASET","metadata":{}},{"cell_type":"code","source":"selected_index = torch.zeros((n_train, ), dtype=bool)\nselected_index[train_indexes] = True\nloader_features_train = torch.utils.data.DataLoader(dataset_features, batch_size=32, shuffle=False)\nloader_pr_train = torch.utils.data.DataLoader(pr_data, batch_size=32, shuffle=False)\nloader_rt_train = torch.utils.data.DataLoader(rt_data, batch_size=32, shuffle=False)\nselected_index = torch.ones((n_train, ), dtype=bool)\nselected_index[train_indexes] = False","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:24.878528Z","iopub.execute_input":"2022-05-12T09:47:24.878935Z","iopub.status.idle":"2022-05-12T09:47:24.886349Z","shell.execute_reply.started":"2022-05-12T09:47:24.878898Z","shell.execute_reply":"2022-05-12T09:47:24.885720Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def init_layer(m):\n    torch.nn.init.xavier_normal_(m.weight, gain=torch.nn.init.calculate_gain('tanh'))\n    torch.nn.init.constant_(m.bias, 0)\n    return m\n\nclass Network(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.dropout = nn.Dropout(p=0.5)\n        self.tanh = nn.Tanh()\n        self.ln1 = init_layer(nn.Linear(4, 8))\n        self.ln2 = init_layer(nn.Linear(8, 8))\n        self.ln3 = init_layer(nn.Linear(8, 1))\n        self.sigmoid = nn.Sigmoid()\n        self.pr_model = TransformerModel(len(vocab_pr), 32, 4, 64, 3)\n        self.rt_model = TransformerModel(len(vocab_rt), 32, 4, 64, 3)\n        self.decoder_pr = init_layer(nn.Linear(32, 1))\n        self.decoder_rt = init_layer(nn.Linear(32, 1))\n        \n    def forward(self, x):\n        x = self.dropout(self.tanh(self.ln1(x)))\n        x = self.dropout(self.tanh(self.ln2(x)))\n        return self.sigmoid(self.ln3(x))\n    \n    def forward_rt(self, x, mask):\n        return self.decoder_rt(self.rt_model(x, mask).mean(dim=1))\n    \n    def forward_pr(self, x, mask):\n        return self.decoder_pr(self.pr_model(x, mask).mean(dim=1))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:24.887764Z","iopub.execute_input":"2022-05-12T09:47:24.888314Z","iopub.status.idle":"2022-05-12T09:47:24.903554Z","shell.execute_reply.started":"2022-05-12T09:47:24.888276Z","shell.execute_reply":"2022-05-12T09:47:24.902704Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"!pip install mlm-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:24.904900Z","iopub.execute_input":"2022-05-12T09:47:24.905303Z","iopub.status.idle":"2022-05-12T09:47:33.787722Z","shell.execute_reply.started":"2022-05-12T09:47:24.905267Z","shell.execute_reply":"2022-05-12T09:47:33.786771Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"from mlm_pytorch import MLM\n\ndef self_init_layer(m):\n    m.weight.data.uniform_(-0.1, 0.1)\n    torch.nn.init.constant_(m.bias, 0)\n    return m\n\nclass myTransformer(nn.Module):\n    \n    def __init__(self, vocab_len=len(vocab_pr)):\n        super().__init__()\n        self.model = TransformerModel(vocab_len, 32, 4, 64, 3)\n        self.decoder = self_init_layer(nn.Linear(32, vocab_len))\n        \n    def forward(self, x, mask=None):\n        src_mask = generate_square_subsequent_mask(x.size(0))\n        if torch.cuda.is_available():\n            src_mask = src_mask.cuda()\n        return self.decoder(self.model(x, src_mask))\n\n\ntransformer_pr = myTransformer(len(vocab_pr))\ntransformer_pr.train()\ntrainer = MLM(\n    transformer_pr,\n    mask_token_id = 1,          # the token id reserved for masking\n    pad_token_id = 0,           # the token id for padding\n    mask_prob = 0.2,           # masking probability for masked language modeling\n    replace_prob = 0.90,        \n    mask_ignore_token_ids = [0]  \n).cuda()\nopt = torch.optim.Adam(trainer.parameters(), lr=3e-4)\nnb_iterations = 50\nfor j in range(nb_iterations):\n    training_loss = 0\n    for batch_idx, data in enumerate(zip(loader_pr_train, loader_rt_train, loader_features_train)):\n        opt.zero_grad()\n        prs, rts, (num_features, label) = data\n        prs = prs.cuda()\n        loss = trainer(prs)\n        loss.backward()\n        opt.step()\n        training_loss += loss.cpu().item()\n    print(f'iter {j} training loss {training_loss}')\n    \ntransformer_rt = myTransformer(len(vocab_rt))\ntransformer_rt.train()\ntrainer = MLM(\n    transformer_rt,\n    mask_token_id = 1,          # the token id reserved for masking\n    pad_token_id = 0,           # the token id for padding\n    mask_prob = 0.2,           # masking probability for masked language modeling\n    replace_prob = 0.90,        \n    mask_ignore_token_ids = [0]  \n).cuda()\nopt = torch.optim.Adam(trainer.parameters(), lr=3e-4)\nnb_iterations = 50\nfor j in range(nb_iterations):\n    training_loss = 0\n    for batch_idx, data in enumerate(zip(loader_pr_train, loader_rt_train, loader_features_train)):\n        opt.zero_grad()\n        prs, rts, (num_features, label) = data\n        loss = trainer(rts.cuda())\n        loss.backward()\n        opt.step()\n        training_loss += loss.cpu().item()\n    print(f'iter {j} training loss {training_loss}')","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:47:33.789454Z","iopub.execute_input":"2022-05-12T09:47:33.789796Z","iopub.status.idle":"2022-05-12T09:48:25.604913Z","shell.execute_reply.started":"2022-05-12T09:47:33.789752Z","shell.execute_reply":"2022-05-12T09:48:25.604104Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"net = Network()\nnet.pr_model = transformer_pr.model\nnet.rt_model = transformer_rt.model\nnet = net.cuda()\n\ncriterion=torch.nn.BCELoss(reduction='none')\n\nnb_iterations = 50\n\noptimizer = torch.optim.Adam(net.parameters(), lr=2e-4)\n\nfor j in range(nb_iterations):\n    correctly_predicted = 0\n    total_prediction = 0\n    training_loss = 0\n    src_mask = generate_square_subsequent_mask(32).cuda()\n    for batch_idx, data in enumerate(zip(loader_pr_train, loader_rt_train, loader_features_train)):\n        optimizer.zero_grad()\n        prs, rts, (num_features, label) = data\n        prs = prs.cuda(non_blocking=True)\n        rts = rts.cuda(non_blocking=True)\n        num_features = num_features.cuda(non_blocking=True)\n        label = label.cuda(non_blocking=True)\n        if rts.size(0) != 32:  # only on last batch\n            src_mask = src_mask[:rts.size(0), :rts.size(0)]\n        prs = net.sigmoid(net.forward_pr(prs, src_mask))\n        rts = net.sigmoid(net.forward_rt(rts, src_mask))\n        smoothed_label = label * 0.8 + 0.1\n        loss = criterion(prs.view(-1), smoothed_label) + criterion(rts.view(-1), smoothed_label)\n        coefficient = torch.ones_like(loss).cuda()\n        coefficient[label == True] += 3\n        loss = loss * coefficient\n        loss = loss.mean()\n        loss.backward()\n        optimizer.step()\n        training_loss += loss.detach().cpu().item()\n    print(f'iter {j} training loss {training_loss}')\n\nnb_iterations = 400\n\noptimizer = torch.optim.AdamW(net.parameters(), lr=3e-4, weight_decay=3e-3)\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, nb_iterations)\n\nnet.train()\nfor j in range(nb_iterations):\n    correctly_predicted = 0\n    total_prediction = 0\n    training_loss = 0\n    src_mask = generate_square_subsequent_mask(32).cuda()\n    for batch_idx, data in enumerate(zip(loader_pr_train, loader_rt_train, loader_features_train)):\n        optimizer.zero_grad()\n        prs, rts, (num_features, label) = data\n        prs = prs.cuda(non_blocking=True)\n        rts = rts.cuda(non_blocking=True)\n        num_features = num_features.cuda(non_blocking=True)\n        label = label.cuda(non_blocking=True)\n        if rts.size(0) != 32:  # only on last batch\n            src_mask = src_mask[:rts.size(0), :rts.size(0)]\n        prs = net.forward_pr(prs, src_mask)\n        rts = net.forward_rt(rts, src_mask)\n        x = torch.hstack((num_features, prs, rts))\n        output = net(x)\n        smoothed_label = label * 0.8 + 0.1\n        loss = criterion(output.view(-1), smoothed_label)\n        coefficient = torch.ones_like(loss).cuda()\n        coefficient[label == True] += 3\n        loss = loss * coefficient\n        loss = loss.mean()\n        loss.backward()\n        optimizer.step()\n        training_loss += loss.detach().cpu().item()\n        predicted = torch.ge(output, 0.5).view(-1)\n        correctly_predicted += torch.sum(label == predicted).detach().cpu()\n        total_prediction += output.shape[0]\n    scheduler.step()\n    print(f'iter {j} training loss {training_loss} accuracy training {correctly_predicted / total_prediction}')","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:48:25.606575Z","iopub.execute_input":"2022-05-12T09:48:25.606890Z","iopub.status.idle":"2022-05-12T09:55:25.269230Z","shell.execute_reply.started":"2022-05-12T09:48:25.606843Z","shell.execute_reply":"2022-05-12T09:55:25.268464Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"# Test data","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/modded-test/test_data_mod.csv')\ntrain_data.head()\nlabels = torch.tensor(test_data[\"Resp\"].values, dtype=torch.float)\nn_labels = labels.shape[0]\nn_train = train_data.shape[0]\nall_features = test_data.iloc[:, 2:]\n# one can assume if Seqs are not present it is a bad sign for survival\n#all_features[\"PR SeqNan\"] = all_features[\"PR Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\n#all_features[\"RT SeqNan\"] = all_features[\"RT Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\nnumeric_features = all_features.dtypes[(all_features.dtypes != 'object') & (all_features.dtypes != 'bool')].index\nmean_numerical_features = all_features[numeric_features].mean()\nstd_numerical_features = all_features[numeric_features].std()\nall_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / x.std() + 1e-4)\nvt_mean = all_features[\"VL-t0\"].mean()\ncd4_mean = all_features[\"CD4-t0\"].mean()\nall_features[\"VL-t0\"] = all_features[\"VL-t0\"].fillna(vt_mean)\nall_features[\"CD4-t0\"] = all_features[\"CD4-t0\"].fillna(cd4_mean)\nall_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:55:25.270808Z","iopub.execute_input":"2022-05-12T09:55:25.271093Z","iopub.status.idle":"2022-05-12T09:55:25.317314Z","shell.execute_reply.started":"2022-05-12T09:55:25.271057Z","shell.execute_reply":"2022-05-12T09:55:25.316284Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"def f_comma(my_str, group=3, char=','):\n    if not pd.isna(my_str):\n        my_str = str(my_str)\n        return char.join(my_str[i:i+group] for i in range(0, len(my_str), group))\n    return ''\n\nfor index, row in all_features.iterrows():\n    all_features['PR Seq'] = all_features['PR Seq'].replace([row['PR Seq']], f_comma(row['PR Seq']))\n    all_features['RT Seq'] = all_features['RT Seq'].replace([row['RT Seq']], f_comma(row['RT Seq']))\n    \nall_features[\"PR Seq\"] = all_features[\"PR Seq\"].apply(lambda x: vocab_pr[tokenize_line(x)])\nall_features[\"RT Seq\"] = all_features[\"RT Seq\"].apply(lambda x: vocab_rt[tokenize_line(x)])\npr_data, pr_length = padding_input(all_features[\"PR Seq\"].values)\nrt_data, rt_length = padding_input(all_features[\"RT Seq\"].values)\n\nnumerical_features = torch.tensor(all_features.iloc[:, 2:].astype('float').values, dtype=torch.float32)\ndataset_features = torch.utils.data.TensorDataset(numerical_features, labels)\nloader_features_train = torch.utils.data.DataLoader(dataset_features, batch_size=32, shuffle=False)\nloader_pr_train = torch.utils.data.DataLoader(pr_data, batch_size=32, shuffle=False)\nloader_rt_train = torch.utils.data.DataLoader(rt_data, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:55:25.318556Z","iopub.execute_input":"2022-05-12T09:55:25.318975Z","iopub.status.idle":"2022-05-12T09:55:27.547210Z","shell.execute_reply.started":"2022-05-12T09:55:25.318937Z","shell.execute_reply":"2022-05-12T09:55:27.546341Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"torch.save(net.state_dict(), 'model.pt')\nnet = net.cuda()\nnet.eval()\ncorrectly_predicted = 0\ntotal_prediction = 0\nsrc_mask = generate_square_subsequent_mask(32).cuda()\nwith torch.no_grad():\n    for batch_idx, data in enumerate(zip(loader_pr_train, loader_rt_train, loader_features_train)):\n        prs, rts, (num_features, label) = data\n        prs = prs.cuda(non_blocking=True)\n        rts = rts.cuda(non_blocking=True)\n        num_features = num_features.cuda(non_blocking=True)\n        label = label.cuda(non_blocking=True)\n        if rts.size(0) != 32:  # only on last batch\n            src_mask = src_mask[:rts.size(0), :rts.size(0)]\n        prs = net.forward_pr(prs, src_mask)\n        rts = net.forward_rt(rts, src_mask)\n        x = torch.hstack((num_features, prs, rts))\n        output = net(x)\n        predicted = torch.ge(output, 0.5).view(-1)\n        correctly_predicted += torch.sum(label == predicted).cpu()\n        total_prediction += output.shape[0]\n    print(f'Test accuracy {correctly_predicted / total_prediction}')","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:55:27.548389Z","iopub.execute_input":"2022-05-12T09:55:27.548649Z","iopub.status.idle":"2022-05-12T09:55:27.708706Z","shell.execute_reply.started":"2022-05-12T09:55:27.548616Z","shell.execute_reply":"2022-05-12T09:55:27.707944Z"},"trusted":true},"execution_count":120,"outputs":[]}]}