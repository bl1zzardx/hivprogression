{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-11T12:43:07.372026Z","iopub.execute_input":"2022-05-11T12:43:07.372332Z","iopub.status.idle":"2022-05-11T12:43:07.383486Z","shell.execute_reply.started":"2022-05-11T12:43:07.372301Z","shell.execute_reply":"2022-05-11T12:43:07.382581Z"},"trusted":true},"execution_count":253,"outputs":[]},{"cell_type":"markdown","source":"# Preliminaries","metadata":{}},{"cell_type":"code","source":"np.random.seed(0)\nrandom.seed(0)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:07.401586Z","iopub.execute_input":"2022-05-11T12:43:07.402335Z","iopub.status.idle":"2022-05-11T12:43:07.406751Z","shell.execute_reply.started":"2022-05-11T12:43:07.402284Z","shell.execute_reply":"2022-05-11T12:43:07.405882Z"},"trusted":true},"execution_count":254,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, Tensor\nfrom torch import optim\nfrom torch.utils import data\nimport wandb\n#wandb.init(project=\"HIV_kaggle\")","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:07.456888Z","iopub.execute_input":"2022-05-11T12:43:07.457523Z","iopub.status.idle":"2022-05-11T12:43:07.461723Z","shell.execute_reply.started":"2022-05-11T12:43:07.457480Z","shell.execute_reply":"2022-05-11T12:43:07.461094Z"},"trusted":true},"execution_count":255,"outputs":[]},{"cell_type":"markdown","source":"Parse and look at first 5 rows","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/hivprogression/training_data.csv')\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:07.656037Z","iopub.execute_input":"2022-05-11T12:43:07.656352Z","iopub.status.idle":"2022-05-11T12:43:07.689148Z","shell.execute_reply.started":"2022-05-11T12:43:07.656318Z","shell.execute_reply":"2022-05-11T12:43:07.688235Z"},"trusted":true},"execution_count":256,"outputs":[]},{"cell_type":"code","source":"labels = torch.tensor(train_data[\"Resp\"].values, dtype=torch.float)\nn_labels = labels.shape[0]\nn_labels","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:07.754788Z","iopub.execute_input":"2022-05-11T12:43:07.755106Z","iopub.status.idle":"2022-05-11T12:43:07.762145Z","shell.execute_reply.started":"2022-05-11T12:43:07.755073Z","shell.execute_reply":"2022-05-11T12:43:07.761509Z"},"trusted":true},"execution_count":257,"outputs":[]},{"cell_type":"code","source":"n_train = train_data.shape[0]\nn_train","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:07.880063Z","iopub.execute_input":"2022-05-11T12:43:07.880499Z","iopub.status.idle":"2022-05-11T12:43:07.885482Z","shell.execute_reply.started":"2022-05-11T12:43:07.880466Z","shell.execute_reply":"2022-05-11T12:43:07.884888Z"},"trusted":true},"execution_count":258,"outputs":[]},{"cell_type":"markdown","source":"We have to remove the first two columns","metadata":{}},{"cell_type":"code","source":"\nall_features = train_data.iloc[:, 2:]\n# one can assume if Seqs are not present it is a bad sign for survival\nall_features[\"PR SeqNan\"] = all_features[\"PR Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\nall_features[\"RT SeqNan\"] = all_features[\"RT Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\nnumeric_features = all_features.dtypes[(all_features.dtypes != 'object') & (all_features.dtypes != 'bool')].index\nmean_numerical_features = all_features[numeric_features].mean()\nstd_numerical_features = all_features[numeric_features].std()\nall_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / x.std() + 1e-4)\nvt_mean = all_features[\"VL-t0\"].mean()\ncd4_mean = all_features[\"CD4-t0\"].mean()\nall_features[\"VL-t0\"] = all_features[\"VL-t0\"].fillna(vt_mean)\nall_features[\"CD4-t0\"] = all_features[\"CD4-t0\"].fillna(cd4_mean)\nall_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:08.107764Z","iopub.execute_input":"2022-05-11T12:43:08.108229Z","iopub.status.idle":"2022-05-11T12:43:08.142784Z","shell.execute_reply.started":"2022-05-11T12:43:08.108195Z","shell.execute_reply":"2022-05-11T12:43:08.142185Z"},"trusted":true},"execution_count":259,"outputs":[]},{"cell_type":"code","source":"def f_comma(my_str, group=3, char=','):\n    if not pd.isna(my_str):\n        my_str = str(my_str)\n        return char.join(my_str[i:i+group] for i in range(0, len(my_str), group))\n    return ''\n\nfor index, row in all_features.iterrows():\n    all_features['PR Seq'] = all_features['PR Seq'].replace([row['PR Seq']], f_comma(row['PR Seq']))\n    all_features['RT Seq'] = all_features['RT Seq'].replace([row['RT Seq']], f_comma(row['RT Seq']))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:08.227648Z","iopub.execute_input":"2022-05-11T12:43:08.228295Z","iopub.status.idle":"2022-05-11T12:43:09.721305Z","shell.execute_reply.started":"2022-05-11T12:43:08.228249Z","shell.execute_reply":"2022-05-11T12:43:09.720239Z"},"trusted":true},"execution_count":260,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize and Vocab","metadata":{}},{"cell_type":"code","source":"import collections\n\ndef tokenize(seqs):\n    return [tokenize_line(seq) for seq in seqs]\n\ndef tokenize_line(seq):\n    if not pd.isna(seq) and len(seq) > 0 and not pd.isna(seq[0]):\n        return list(seq.split(','))\n    return []\n\nclass Vocab:\n    def __init__(self, tokens):\n        counter = count_corpus(tokens)\n        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n                                   reverse=True)\n        self.idx_to_token = ['<unk>']\n        self.token_to_idx = {token: idx\n                             for idx, token in enumerate(self.idx_to_token)}\n        for token, freq in self._token_freqs:\n            if token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) - 1\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\n    @property\n    def unk(self): \n        return 0\n\n    @property\n    def token_freqs(self):\n        return self._token_freqs\n\ndef count_corpus(tokens):\n    if len(tokens) == 0 or isinstance(tokens[0], list):\n        tokens = [token for line in tokens for token in line]\n    return collections.Counter(tokens)\n\ntokens_pr = tokenize(all_features[\"PR Seq\"].values)\nvocab_pr = Vocab(tokens_pr)\n#list(vocab_pr.token_to_idx.items())","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:09.723171Z","iopub.execute_input":"2022-05-11T12:43:09.723434Z","iopub.status.idle":"2022-05-11T12:43:09.767481Z","shell.execute_reply.started":"2022-05-11T12:43:09.723402Z","shell.execute_reply":"2022-05-11T12:43:09.766364Z"},"trusted":true},"execution_count":261,"outputs":[]},{"cell_type":"code","source":"all_features[\"PR Seq\"] = all_features[\"PR Seq\"].apply(lambda x: vocab_pr[tokenize_line(x)])\nall_features[\"PR Seq\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:09.768897Z","iopub.execute_input":"2022-05-11T12:43:09.769242Z","iopub.status.idle":"2022-05-11T12:43:09.873594Z","shell.execute_reply.started":"2022-05-11T12:43:09.769196Z","shell.execute_reply":"2022-05-11T12:43:09.872678Z"},"trusted":true},"execution_count":262,"outputs":[]},{"cell_type":"code","source":"tokens_rt = tokenize(all_features[\"RT Seq\"].values)\nvocab_rt = Vocab(tokens_rt)\n#list(vocab_rt.token_to_idx.items())","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:09.875821Z","iopub.execute_input":"2022-05-11T12:43:09.876055Z","iopub.status.idle":"2022-05-11T12:43:10.501459Z","shell.execute_reply.started":"2022-05-11T12:43:09.876027Z","shell.execute_reply":"2022-05-11T12:43:10.500317Z"},"trusted":true},"execution_count":263,"outputs":[]},{"cell_type":"code","source":"all_features[\"RT Seq\"] = all_features[\"RT Seq\"].apply(lambda x: vocab_rt[tokenize_line(x)])\nall_features[\"RT Seq\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:10.502876Z","iopub.execute_input":"2022-05-11T12:43:10.503831Z","iopub.status.idle":"2022-05-11T12:43:10.791084Z","shell.execute_reply.started":"2022-05-11T12:43:10.503764Z","shell.execute_reply":"2022-05-11T12:43:10.790064Z"},"trusted":true},"execution_count":264,"outputs":[]},{"cell_type":"markdown","source":"# Network","metadata":{}},{"cell_type":"code","source":"import math\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.0, max_len: int = 1000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n                 nlayers: int, dropout: float = 0.4):\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, d_model)\n        self.d_model = d_model\n        self.decoder = nn.Linear(d_model, 1)\n        self.init_weights()\n\n    def init_weights(self) -> None:\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n        src = self.encoder(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_mask).mean(dim=1)\n        output = self.decoder(output)\n        return output\n    \ndef generate_square_subsequent_mask(sz: int) -> Tensor:\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:10.792598Z","iopub.execute_input":"2022-05-11T12:43:10.792850Z","iopub.status.idle":"2022-05-11T12:43:10.810778Z","shell.execute_reply.started":"2022-05-11T12:43:10.792821Z","shell.execute_reply":"2022-05-11T12:43:10.809779Z"},"trusted":true},"execution_count":265,"outputs":[]},{"cell_type":"markdown","source":"# Performance optimization => batching","metadata":{}},{"cell_type":"markdown","source":"Padding text inputs","metadata":{}},{"cell_type":"code","source":"def padding_input(seqs):\n    # determine max length\n    max_length = 0\n    for seq in seqs:\n        max_length = max(max_length, len(seq))\n    result = torch.zeros((seqs.shape[0], max_length), dtype=int)\n    for i in range(seqs.shape[0]):\n        for j in range(len(seqs[i])):\n            result[i][j] = seqs[i][j]\n    return result, max_length","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:10.811874Z","iopub.execute_input":"2022-05-11T12:43:10.812551Z","iopub.status.idle":"2022-05-11T12:43:10.830927Z","shell.execute_reply.started":"2022-05-11T12:43:10.812507Z","shell.execute_reply":"2022-05-11T12:43:10.829869Z"},"trusted":true},"execution_count":266,"outputs":[]},{"cell_type":"code","source":"pr_data, pr_length = padding_input(all_features[\"PR Seq\"].values)\nrt_data, rt_length = padding_input(all_features[\"RT Seq\"].values)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:10.832476Z","iopub.execute_input":"2022-05-11T12:43:10.832718Z","iopub.status.idle":"2022-05-11T12:43:12.658119Z","shell.execute_reply.started":"2022-05-11T12:43:10.832690Z","shell.execute_reply":"2022-05-11T12:43:12.657221Z"},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"markdown","source":"Dataset and DataLoaders","metadata":{}},{"cell_type":"code","source":"training_size = int(0.7 * n_train)\ntrain_indexes = np.random.choice(n_train, training_size)\nnumerical_features = torch.tensor(all_features.iloc[:, 2:].astype('float').values, dtype=torch.float32)\ndataset_features = torch.utils.data.TensorDataset(numerical_features, labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:12.701244Z","iopub.execute_input":"2022-05-11T12:43:12.701553Z","iopub.status.idle":"2022-05-11T12:43:12.709394Z","shell.execute_reply.started":"2022-05-11T12:43:12.701520Z","shell.execute_reply":"2022-05-11T12:43:12.708260Z"},"trusted":true},"execution_count":268,"outputs":[]},{"cell_type":"markdown","source":"# TO FIX : SPLITTING DATASET","metadata":{}},{"cell_type":"code","source":"selected_index = torch.zeros((n_train, ), dtype=bool)\nselected_index[train_indexes] = True\nloader_features_train = torch.utils.data.DataLoader(dataset_features, batch_size=32, shuffle=False)\nloader_pr_train = torch.utils.data.DataLoader(pr_data, batch_size=32, shuffle=False)\nloader_rt_train = torch.utils.data.DataLoader(rt_data, batch_size=32, shuffle=False)\nselected_index = torch.ones((n_train, ), dtype=bool)\nselected_index[train_indexes] = False","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:12.712497Z","iopub.execute_input":"2022-05-11T12:43:12.712748Z","iopub.status.idle":"2022-05-11T12:43:12.725469Z","shell.execute_reply.started":"2022-05-11T12:43:12.712719Z","shell.execute_reply":"2022-05-11T12:43:12.724754Z"},"trusted":true},"execution_count":269,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def init_layer(m):\n    torch.nn.init.xavier_normal_(m.weight, gain=torch.nn.init.calculate_gain('tanh'))\n    torch.nn.init.constant_(m.bias, 0)\n    return m\n\nclass Network(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.dropout = nn.Dropout(p=0.3)\n        self.tanh = nn.Tanh()\n        self.ln1 = init_layer(nn.Linear(6, 12))\n        self.ln2 = init_layer(nn.Linear(12, 12))\n        self.ln3 = init_layer(nn.Linear(12, 1))\n        self.sigmoid = nn.Sigmoid()\n        self.pr_model = TransformerModel(len(vocab_pr), 16, 4, 32, 4)\n        self.rt_model = TransformerModel(len(vocab_rt), 16, 4, 32, 4)\n        \n    def forward(self, x):\n        x = self.dropout(self.tanh(self.ln1(x)))\n        x = self.dropout(self.tanh(self.ln2(x)))\n        return self.sigmoid(self.ln3(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:12.727050Z","iopub.execute_input":"2022-05-11T12:43:12.728086Z","iopub.status.idle":"2022-05-11T12:43:12.740342Z","shell.execute_reply.started":"2022-05-11T12:43:12.728036Z","shell.execute_reply":"2022-05-11T12:43:12.739433Z"},"trusted":true},"execution_count":270,"outputs":[]},{"cell_type":"code","source":"net = Network()\n\ncriterion=torch.nn.BCELoss(reduction='none')\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n\nnb_iterations = 500\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, nb_iterations)\n\nnet.train()\nfor j in range(nb_iterations):\n    correctly_predicted = 0\n    total_prediction = 0\n    training_loss = 0\n    src_mask = generate_square_subsequent_mask(32)\n    for batch_idx, data in enumerate(zip(loader_pr_train, loader_rt_train, loader_features_train)):\n        optimizer.zero_grad()\n        prs, rts, (num_features, label) = data\n        if rts.size(0) != 32:  # only on last batch\n            src_mask = src_mask[:rts.size(0), :rts.size(0)]\n        prs = net.pr_model(prs, src_mask)\n        rts = net.rt_model(rts, src_mask)\n        x = torch.hstack((num_features, prs, rts))\n        output = net(x)\n        loss = criterion(output.view(-1), label)\n        coefficient = torch.ones_like(loss)\n        coefficient[label == True] += 3\n        loss = loss * coefficient\n        loss = loss.mean()\n        loss.backward()\n        optimizer.step()\n        training_loss += loss.item()\n        predicted = torch.ge(output, 0.5).view(-1)\n        correctly_predicted += torch.sum(label == predicted)\n        total_prediction += output.shape[0]\n    scheduler.step()\n    print(f'iter {j} training loss {training_loss} accuracy training {correctly_predicted / total_prediction}')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T12:43:12.742099Z","iopub.execute_input":"2022-05-11T12:43:12.742463Z","iopub.status.idle":"2022-05-11T13:01:00.376354Z","shell.execute_reply.started":"2022-05-11T12:43:12.742417Z","shell.execute_reply":"2022-05-11T13:01:00.375088Z"},"trusted":true},"execution_count":271,"outputs":[]},{"cell_type":"markdown","source":"# Test data","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/modded-test/test_data_mod.csv')\ntrain_data.head()\nlabels = torch.tensor(test_data[\"Resp\"].values, dtype=torch.float)\nn_labels = labels.shape[0]\nn_train = train_data.shape[0]\nall_features = test_data.iloc[:, 2:]\n# one can assume if Seqs are not present it is a bad sign for survival\nall_features[\"PR SeqNan\"] = all_features[\"PR Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\nall_features[\"RT SeqNan\"] = all_features[\"RT Seq\"].apply(lambda x: pd.isna(x)).astype(bool)\nnumeric_features = all_features.dtypes[(all_features.dtypes != 'object') & (all_features.dtypes != 'bool')].index\nmean_numerical_features = all_features[numeric_features].mean()\nstd_numerical_features = all_features[numeric_features].std()\nall_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / x.std() + 1e-4)\nvt_mean = all_features[\"VL-t0\"].mean()\ncd4_mean = all_features[\"CD4-t0\"].mean()\nall_features[\"VL-t0\"] = all_features[\"VL-t0\"].fillna(vt_mean)\nall_features[\"CD4-t0\"] = all_features[\"CD4-t0\"].fillna(cd4_mean)\nall_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T13:01:03.754985Z","iopub.execute_input":"2022-05-11T13:01:03.755633Z","iopub.status.idle":"2022-05-11T13:01:03.829054Z","shell.execute_reply.started":"2022-05-11T13:01:03.755580Z","shell.execute_reply":"2022-05-11T13:01:03.827965Z"},"trusted":true},"execution_count":272,"outputs":[]},{"cell_type":"code","source":"def f_comma(my_str, group=3, char=','):\n    if not pd.isna(my_str):\n        my_str = str(my_str)\n        return char.join(my_str[i:i+group] for i in range(0, len(my_str), group))\n    return ''\n\nfor index, row in all_features.iterrows():\n    all_features['PR Seq'] = all_features['PR Seq'].replace([row['PR Seq']], f_comma(row['PR Seq']))\n    all_features['RT Seq'] = all_features['RT Seq'].replace([row['RT Seq']], f_comma(row['RT Seq']))\n    \nall_features[\"PR Seq\"] = all_features[\"PR Seq\"].apply(lambda x: vocab_pr[tokenize_line(x)])\nall_features[\"RT Seq\"] = all_features[\"RT Seq\"].apply(lambda x: vocab_rt[tokenize_line(x)])\npr_data, pr_length = padding_input(all_features[\"PR Seq\"].values)\nrt_data, rt_length = padding_input(all_features[\"RT Seq\"].values)\n\nnumerical_features = torch.tensor(all_features.iloc[:, 2:].astype('float').values, dtype=torch.float32)\ndataset_features = torch.utils.data.TensorDataset(numerical_features, labels)\nloader_features_train = torch.utils.data.DataLoader(dataset_features, batch_size=32, shuffle=False)\nloader_pr_train = torch.utils.data.DataLoader(pr_data, batch_size=32, shuffle=False)\nloader_rt_train = torch.utils.data.DataLoader(rt_data, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T13:01:06.531636Z","iopub.execute_input":"2022-05-11T13:01:06.531979Z","iopub.status.idle":"2022-05-11T13:01:09.221373Z","shell.execute_reply.started":"2022-05-11T13:01:06.531940Z","shell.execute_reply":"2022-05-11T13:01:09.220372Z"},"trusted":true},"execution_count":273,"outputs":[]},{"cell_type":"code","source":"torch.save(net.state_dict(), 'model.pt')\nnet.eval()\ncorrectly_predicted = 0\ntotal_prediction = 0\nsrc_mask = generate_square_subsequent_mask(32)\nfor batch_idx, data in enumerate(zip(loader_pr_train, loader_rt_train, loader_features_train)):\n    prs, rts, (num_features, label) = data\n    if rts.size(0) != 32:  # only on last batch\n        src_mask = src_mask[:rts.size(0), :rts.size(0)]\n    prs = net.pr_model(prs, src_mask)\n    rts = net.rt_model(rts, src_mask)\n    x = torch.hstack((num_features, prs, rts))\n    output = net(x)\n    predicted = torch.ge(output, 0.5).view(-1)\n    correctly_predicted += torch.sum(label == predicted)\n    total_prediction += output.shape[0]\nprint(f'Test accuracy {correctly_predicted / total_prediction}')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T13:02:18.013107Z","iopub.execute_input":"2022-05-11T13:02:18.013608Z","iopub.status.idle":"2022-05-11T13:02:19.754566Z","shell.execute_reply.started":"2022-05-11T13:02:18.013557Z","shell.execute_reply":"2022-05-11T13:02:19.753651Z"},"trusted":true},"execution_count":276,"outputs":[]}]}